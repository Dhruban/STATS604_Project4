{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weather Forecast Data Download (Nov 20-29, 2025)\n",
    "\n",
    "This notebook downloads hourly weather **forecast** data from Open-Meteo API for multiple load areas across the PJM region.\n",
    "Data is retrieved in UTC and converted to Eastern Time (ET).\n",
    "\n",
    "**Date Range**: November 20, 2025 00:00 UTC to November 29, 2025 23:00 UTC  \n",
    "**Note**: Uses forecast API for future dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import pytz\n",
    "import os\n",
    "import time\n",
    "import holidays\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import glob\n",
    "import warnings\n",
    "import holidays\n",
    "import pickle\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from xgboost import XGBRegressor, XGBClassifier\n",
    "from lightgbm import LGBMRegressor, LGBMClassifier\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from tqdm import tqdm\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define Load Area Coordinates\n",
    "\n",
    "Define the latitude and longitude for each load area in the PJM region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total load areas: 29\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>load_area</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AECO</td>\n",
       "      <td>39.45</td>\n",
       "      <td>-74.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AEPAPT</td>\n",
       "      <td>37.25</td>\n",
       "      <td>-81.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AEPIMP</td>\n",
       "      <td>38.45</td>\n",
       "      <td>-81.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AEPKPT</td>\n",
       "      <td>38.20</td>\n",
       "      <td>-83.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AEPOPT</td>\n",
       "      <td>39.90</td>\n",
       "      <td>-82.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  load_area    lat   lon\n",
       "0      AECO  39.45 -74.5\n",
       "1    AEPAPT  37.25 -81.3\n",
       "2    AEPIMP  38.45 -81.6\n",
       "3    AEPKPT  38.20 -83.1\n",
       "4    AEPOPT  39.90 -82.9"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zone_coords = pd.DataFrame({\n",
    "    'load_area': ['AECO', 'AEPAPT', 'AEPIMP', 'AEPKPT', 'AEPOPT', 'AP', 'BC', 'CE', 'DAY', 'DEOK',\n",
    "                  'DOM', 'DPLCO', 'DUQ', 'EASTON', 'EKPC', 'JC', 'ME', 'OE', 'OVEC', 'PAPWR',\n",
    "                  'PE', 'PEPCO', 'PLCO', 'PN', 'PS', 'RECO', 'SMECO', 'UGI', 'VMEU'],\n",
    "    'lat': [39.45, 37.25, 38.45, 38.20, 39.90, 37.30, 40.80, 41.85, 39.75, 39.10,\n",
    "            37.55, 38.90, 40.45, 39.55, 37.75, 40.35, 40.20, 41.10, 38.85, 40.70,\n",
    "            40.00, 38.90, 40.95, 41.15, 40.75, 41.00, 38.40, 40.25, 37.30],\n",
    "    'lon': [-74.50, -81.30, -81.60, -83.10, -82.90, -80.90, -79.95, -86.10, -84.20, -84.50,\n",
    "            -77.45, -75.50, -79.90, -75.10, -84.30, -74.65, -76.00, -81.25, -82.85, -77.80,\n",
    "            -75.20, -76.95, -77.40, -77.80, -74.15, -74.10, -76.70, -75.65, -76.00]\n",
    "})\n",
    "\n",
    "print(f\"Total load areas: {len(zone_coords)}\")\n",
    "zone_coords.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configure API Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date range: 2025-11-20 to 2025-11-30\n",
      "Variables: temperature_2m,relative_humidity_2m,dew_point_2m,precipitation,wind_speed_10m\n",
      "Using forecast API for future dates\n"
     ]
    }
   ],
   "source": [
    "# Date range to download\n",
    "start_date = \"2025-11-20\"\n",
    "end_date = \"2025-11-30\"\n",
    "\n",
    "# API endpoint - using forecast API for future dates\n",
    "api = \"https://api.open-meteo.com/v1/forecast\"\n",
    "\n",
    "# Weather variables to retrieve\n",
    "hourly_vars = \"temperature_2m,relative_humidity_2m,dew_point_2m,precipitation,wind_speed_10m\"\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(\"data/weather\", exist_ok=True)\n",
    "\n",
    "print(f\"Date range: {start_date} to {end_date}\")\n",
    "print(f\"Variables: {hourly_vars}\")\n",
    "print(f\"Using forecast API for future dates\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define Weather Data Fetcher Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_weather_data(lat, lon, start_date, end_date, tries=3):\n",
    "    \"\"\"Fetch weather data for a specific date range and location\"\"\"\n",
    "    # For forecast API, we use forecast_days parameter (max 16 days)\n",
    "    url = (f\"{api}?latitude={lat}&longitude={lon}\"\n",
    "           f\"&hourly={hourly_vars}&timezone=UTC&forecast_days=16\")\n",
    "    \n",
    "    for k in range(1, tries + 1):\n",
    "        try:\n",
    "            res = requests.get(url, timeout=60)\n",
    "            if res.status_code == 200:\n",
    "                j = res.json()\n",
    "                if 'hourly' in j and 'time' in j['hourly']:\n",
    "                    df = pd.DataFrame({\n",
    "                        'datetime_beginning_utc': j['hourly']['time'],\n",
    "                        'temp': j['hourly']['temperature_2m'],\n",
    "                        'humidity': j['hourly']['relative_humidity_2m'],\n",
    "                        'dew_point': j['hourly']['dew_point_2m'],\n",
    "                        'precip': j['hourly']['precipitation'],\n",
    "                        'wind': j['hourly']['wind_speed_10m']\n",
    "                    })\n",
    "                    \n",
    "                    # Filter to the specific date range we want\n",
    "                    df['datetime_beginning_utc'] = pd.to_datetime(df['datetime_beginning_utc'])\n",
    "                    mask = (df['datetime_beginning_utc'] >= start_date) & (df['datetime_beginning_utc'] <= end_date + ' 23:00:00')\n",
    "                    df = df[mask].copy()\n",
    "                    df['datetime_beginning_utc'] = df['datetime_beginning_utc'].astype(str)\n",
    "                    \n",
    "                    return df\n",
    "                else:\n",
    "                    print(f\"Unexpected response format for lat={lat}, lon={lon}\")\n",
    "            else:\n",
    "                print(f\"HTTP {res.status_code} for lat={lat}, lon={lon}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error on attempt {k} for lat={lat}, lon={lon}: {e}\")\n",
    "        \n",
    "        if k < tries:\n",
    "            time.sleep(0.7 * k)\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Download Weather Data for All Load Areas\n",
    "\n",
    "This cell downloads weather data for the specified date range across all load areas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading weather data for 29 load areas...\n",
      "\n",
      "============================================================\n",
      "[1/29] Fetching AECO (lat=39.45, lon=-74.5)...\n",
      "  ✓ Successfully fetched 264 hourly records\n",
      "[2/29] Fetching AEPAPT (lat=37.25, lon=-81.3)...\n",
      "  ✓ Successfully fetched 264 hourly records\n",
      "[3/29] Fetching AEPIMP (lat=38.45, lon=-81.6)...\n",
      "  ✓ Successfully fetched 264 hourly records\n",
      "[4/29] Fetching AEPKPT (lat=38.2, lon=-83.1)...\n",
      "  ✓ Successfully fetched 264 hourly records\n",
      "[5/29] Fetching AEPOPT (lat=39.9, lon=-82.9)...\n",
      "  ✓ Successfully fetched 264 hourly records\n",
      "[6/29] Fetching AP (lat=37.3, lon=-80.9)...\n",
      "  ✓ Successfully fetched 264 hourly records\n",
      "[7/29] Fetching BC (lat=40.8, lon=-79.95)...\n",
      "  ✓ Successfully fetched 264 hourly records\n",
      "[8/29] Fetching CE (lat=41.85, lon=-86.1)...\n",
      "  ✓ Successfully fetched 264 hourly records\n",
      "[9/29] Fetching DAY (lat=39.75, lon=-84.2)...\n",
      "  ✓ Successfully fetched 264 hourly records\n",
      "[10/29] Fetching DEOK (lat=39.1, lon=-84.5)...\n",
      "  ✓ Successfully fetched 264 hourly records\n",
      "[11/29] Fetching DOM (lat=37.55, lon=-77.45)...\n",
      "  ✓ Successfully fetched 264 hourly records\n",
      "[12/29] Fetching DPLCO (lat=38.9, lon=-75.5)...\n",
      "  ✓ Successfully fetched 264 hourly records\n",
      "[13/29] Fetching DUQ (lat=40.45, lon=-79.9)...\n",
      "  ✓ Successfully fetched 264 hourly records\n",
      "[14/29] Fetching EASTON (lat=39.55, lon=-75.1)...\n",
      "  ✓ Successfully fetched 264 hourly records\n",
      "[15/29] Fetching EKPC (lat=37.75, lon=-84.3)...\n",
      "  ✓ Successfully fetched 264 hourly records\n",
      "[16/29] Fetching JC (lat=40.35, lon=-74.65)...\n",
      "  ✓ Successfully fetched 264 hourly records\n",
      "[17/29] Fetching ME (lat=40.2, lon=-76.0)...\n",
      "  ✓ Successfully fetched 264 hourly records\n",
      "[18/29] Fetching OE (lat=41.1, lon=-81.25)...\n",
      "  ✓ Successfully fetched 264 hourly records\n",
      "[19/29] Fetching OVEC (lat=38.85, lon=-82.85)...\n",
      "  ✓ Successfully fetched 264 hourly records\n",
      "[20/29] Fetching PAPWR (lat=40.7, lon=-77.8)...\n",
      "  ✓ Successfully fetched 264 hourly records\n",
      "[21/29] Fetching PE (lat=40.0, lon=-75.2)...\n",
      "  ✓ Successfully fetched 264 hourly records\n",
      "[22/29] Fetching PEPCO (lat=38.9, lon=-76.95)...\n",
      "  ✓ Successfully fetched 264 hourly records\n",
      "[23/29] Fetching PLCO (lat=40.95, lon=-77.4)...\n",
      "  ✓ Successfully fetched 264 hourly records\n",
      "[24/29] Fetching PN (lat=41.15, lon=-77.8)...\n",
      "  ✓ Successfully fetched 264 hourly records\n",
      "[25/29] Fetching PS (lat=40.75, lon=-74.15)...\n",
      "  ✓ Successfully fetched 264 hourly records\n",
      "[26/29] Fetching RECO (lat=41.0, lon=-74.1)...\n",
      "  ✓ Successfully fetched 264 hourly records\n",
      "[27/29] Fetching SMECO (lat=38.4, lon=-76.7)...\n",
      "  ✓ Successfully fetched 264 hourly records\n",
      "[28/29] Fetching UGI (lat=40.25, lon=-75.65)...\n",
      "  ✓ Successfully fetched 264 hourly records\n",
      "[29/29] Fetching VMEU (lat=37.3, lon=-76.0)...\n",
      "  ✓ Successfully fetched 264 hourly records\n",
      "============================================================\n",
      "\n",
      "Data collection complete! Total zones processed: 29/29\n"
     ]
    }
   ],
   "source": [
    "# Initialize list to collect data from all zones\n",
    "all_data = []\n",
    "\n",
    "print(f\"Downloading weather data for {len(zone_coords)} load areas...\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for idx, row in zone_coords.iterrows():\n",
    "    zone = row['load_area']\n",
    "    lat = row['lat']\n",
    "    lon = row['lon']\n",
    "    \n",
    "    print(f\"[{idx+1}/{len(zone_coords)}] Fetching {zone} (lat={lat}, lon={lon})...\")\n",
    "    \n",
    "    # Fetch data for this zone\n",
    "    df = fetch_weather_data(lat, lon, start_date, end_date)\n",
    "    \n",
    "    if df is not None and not df.empty:\n",
    "        # Add load area identifier\n",
    "        df['load_area'] = zone\n",
    "        all_data.append(df)\n",
    "        print(f\"  ✓ Successfully fetched {len(df)} hourly records\")\n",
    "    else:\n",
    "        print(f\"  ✗ Failed to fetch data for {zone}\")\n",
    "    \n",
    "    # Small delay between requests to be respectful to the API\n",
    "    time.sleep(0.5)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nData collection complete! Total zones processed: {len(all_data)}/{len(zone_coords)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Combine Data and Convert to Eastern Time\n",
    "\n",
    "Combine all load area data and convert UTC timestamps to Eastern Time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined dataset shape: (7656, 7)\n",
      "Date range (UTC): 2025-11-20 00:00:00 to 2025-11-30 23:00:00\n",
      "\n",
      "Converting UTC to Eastern Time...\n",
      "✓ Time conversion complete\n",
      "\n",
      "Sample of data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime_beginning_ept</th>\n",
       "      <th>load_area</th>\n",
       "      <th>temp</th>\n",
       "      <th>humidity</th>\n",
       "      <th>dew_point</th>\n",
       "      <th>precip</th>\n",
       "      <th>wind</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11/19/2025 7:00:00 PM</td>\n",
       "      <td>AECO</td>\n",
       "      <td>5.8</td>\n",
       "      <td>90</td>\n",
       "      <td>4.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11/19/2025 8:00:00 PM</td>\n",
       "      <td>AECO</td>\n",
       "      <td>5.5</td>\n",
       "      <td>90</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11/19/2025 9:00:00 PM</td>\n",
       "      <td>AECO</td>\n",
       "      <td>5.3</td>\n",
       "      <td>89</td>\n",
       "      <td>3.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11/19/2025 10:00:00 PM</td>\n",
       "      <td>AECO</td>\n",
       "      <td>5.2</td>\n",
       "      <td>89</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11/19/2025 11:00:00 PM</td>\n",
       "      <td>AECO</td>\n",
       "      <td>5.2</td>\n",
       "      <td>89</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>11/20/2025 12:00:00 AM</td>\n",
       "      <td>AECO</td>\n",
       "      <td>5.2</td>\n",
       "      <td>89</td>\n",
       "      <td>3.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>11/20/2025 1:00:00 AM</td>\n",
       "      <td>AECO</td>\n",
       "      <td>5.2</td>\n",
       "      <td>88</td>\n",
       "      <td>3.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>11/20/2025 2:00:00 AM</td>\n",
       "      <td>AECO</td>\n",
       "      <td>5.0</td>\n",
       "      <td>89</td>\n",
       "      <td>3.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>11/20/2025 3:00:00 AM</td>\n",
       "      <td>AECO</td>\n",
       "      <td>4.9</td>\n",
       "      <td>88</td>\n",
       "      <td>3.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>11/20/2025 4:00:00 AM</td>\n",
       "      <td>AECO</td>\n",
       "      <td>4.6</td>\n",
       "      <td>88</td>\n",
       "      <td>2.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   datetime_beginning_ept load_area  temp  humidity  dew_point  precip  wind\n",
       "0   11/19/2025 7:00:00 PM      AECO   5.8        90        4.3     0.0  14.9\n",
       "1   11/19/2025 8:00:00 PM      AECO   5.5        90        4.0     0.0  13.7\n",
       "2   11/19/2025 9:00:00 PM      AECO   5.3        89        3.6     0.0  12.8\n",
       "3  11/19/2025 10:00:00 PM      AECO   5.2        89        3.5     0.0  12.2\n",
       "4  11/19/2025 11:00:00 PM      AECO   5.2        89        3.5     0.0  11.8\n",
       "5  11/20/2025 12:00:00 AM      AECO   5.2        89        3.6     0.0  10.9\n",
       "6   11/20/2025 1:00:00 AM      AECO   5.2        88        3.4     0.0  12.4\n",
       "7   11/20/2025 2:00:00 AM      AECO   5.0        89        3.3     0.0  10.8\n",
       "8   11/20/2025 3:00:00 AM      AECO   4.9        88        3.1     0.0  10.5\n",
       "9   11/20/2025 4:00:00 AM      AECO   4.6        88        2.8     0.0  10.7"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if all_data:\n",
    "    # Combine all data\n",
    "    combined_df = pd.concat(all_data, ignore_index=True)\n",
    "    \n",
    "    print(f\"Combined dataset shape: {combined_df.shape}\")\n",
    "    print(f\"Date range (UTC): {combined_df['datetime_beginning_utc'].min()} to {combined_df['datetime_beginning_utc'].max()}\")\n",
    "    \n",
    "    # Convert UTC to Eastern Time\n",
    "    print(\"\\nConverting UTC to Eastern Time...\")\n",
    "    combined_df['datetime_beginning_utc'] = pd.to_datetime(combined_df['datetime_beginning_utc'])\n",
    "    \n",
    "    # Create Eastern Time column\n",
    "    utc = pytz.UTC\n",
    "    eastern = pytz.timezone('US/Eastern')\n",
    "    et_time = combined_df['datetime_beginning_utc'].dt.tz_localize(utc).dt.tz_convert(eastern)\n",
    "    \n",
    "    # Format datetime in M/D/YYYY H:MM:SS AM/PM format\n",
    "    # Remove timezone info and format manually for cross-platform compatibility\n",
    "    et_time_no_tz = et_time.dt.tz_localize(None)\n",
    "    \n",
    "    # Try Unix format first, fall back to Windows format if needed\n",
    "    try:\n",
    "        combined_df['datetime_beginning_ept'] = et_time_no_tz.dt.strftime('%-m/%-d/%Y %-I:%M:%S %p')\n",
    "    except:\n",
    "        # Windows format\n",
    "        combined_df['datetime_beginning_ept'] = et_time_no_tz.dt.strftime('%#m/%#d/%Y %#I:%M:%S %p')\n",
    "    \n",
    "    # Remove UTC column and reorder\n",
    "    combined_df = combined_df.drop('datetime_beginning_utc', axis=1)\n",
    "    column_order = ['datetime_beginning_ept', 'load_area', 'temp', 'humidity', 'dew_point', 'precip', 'wind']\n",
    "    combined_df = combined_df[column_order]\n",
    "    \n",
    "    print(\"✓ Time conversion complete\")\n",
    "    \n",
    "    # Display sample\n",
    "    print(\"\\nSample of data:\")\n",
    "    display(combined_df.head(10))\n",
    "else:\n",
    "    print(\"⚠ No data was successfully downloaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "# Directory paths (relative to src/)\n",
    "FIGURES_DIR = \"../figures\"\n",
    "OUTPUT_DIR = \"../output\"\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(FIGURES_DIR, exist_ok=True)\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Target 29 load areas\n",
    "KEEP_AREAS = [\n",
    "    \"AECO\", \"AEPAPT\", \"AEPIMP\", \"AEPKPT\", \"AEPOPT\", \"AP\", \"BC\", \"CE\", \"DAY\", \"DEOK\",\n",
    "    \"DOM\", \"DPLCO\", \"DUQ\", \"EASTON\", \"EKPC\", \"JC\", \"ME\", \"OE\", \"OVEC\", \"PAPWR\",\n",
    "    \"PE\", \"PEPCO\", \"PLCO\", \"PN\", \"PS\", \"RECO\", \"SMECO\", \"UGI\", \"VMEU\"\n",
    "]\n",
    "\n",
    "# Best Model Finding dates (2025 test)\n",
    "TEST_START = '2024-11-20 00:00:00'\n",
    "TEST_END = '2024-11-29 23:00:00'\n",
    "\n",
    "# Rolling window for peak days\n",
    "WINDOW_SIZE = 10  # days\n",
    "NUM_PEAK_DAYS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= combined_df.copy()\n",
    "def parse_et(series):\n",
    "    \"\"\"Parse datetime with Eastern Time timezone handling\"\"\"\n",
    "    if pd.api.types.is_datetime64_any_dtype(series):\n",
    "        result = pd.to_datetime(series)\n",
    "        if result.dt.tz is None:\n",
    "            return result.dt.tz_localize('America/New_York', ambiguous='NaT', nonexistent='NaT')\n",
    "        return result\n",
    "    result = pd.to_datetime(series, format='%m/%d/%Y %I:%M:%S %p', errors='coerce')\n",
    "    mask = result.isna()\n",
    "    if mask.any():\n",
    "        result[mask] = pd.to_datetime(series[mask], errors='coerce')\n",
    "    return result.dt.tz_localize('America/New_York', ambiguous='NaT', nonexistent='NaT')\n",
    "\n",
    "df = df.rename(columns={\n",
    "    'datetime_beginning_ept': 'datetime',\n",
    "    'load_area': 'region',\n",
    "    'temp': 'temperature',\n",
    "    'precip': 'precipitation',\n",
    "    'wind': 'wind_speed'\n",
    "})\n",
    "\n",
    "# Keep only necessary columns\n",
    "keep_cols = ['datetime', 'region', 'temperature', 'humidity', 'precipitation', 'wind_speed']\n",
    "df = df[keep_cols]\n",
    "\n",
    "df['datetime'] = parse_et(df['datetime'])\n",
    "df = df.dropna(subset=['datetime'])\n",
    "df = df.sort_values('datetime').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_features(df):\n",
    "    \"\"\"Add temporal and calendar features\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # ===== TEMPORAL FEATURES =====\n",
    "    df['hour'] = df['datetime'].dt.hour\n",
    "    df['day_of_week'] = df['datetime'].dt.dayofweek  # Monday=0, Sunday=6\n",
    "    df['month'] = df['datetime'].dt.month\n",
    "    df['day_of_month'] = df['datetime'].dt.day\n",
    "    df['day_of_year'] = df['datetime'].dt.dayofyear\n",
    "    df['week_of_year'] = df['datetime'].dt.isocalendar().week\n",
    "    df['is_weekend'] = (df['day_of_week'] >= 5).astype(int)\n",
    "    \n",
    "    # Cyclical encoding\n",
    "    df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)\n",
    "    df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)\n",
    "    df['day_sin'] = np.sin(2 * np.pi * df['day_of_week'] / 7)\n",
    "    df['day_cos'] = np.cos(2 * np.pi * df['day_of_week'] / 7)\n",
    "    \n",
    "    # ===== HOLIDAY FEATURES =====\n",
    "    # Create US holiday calendar\n",
    "    us_holidays = holidays.US(years=range(2016, 2026))\n",
    "    df['date'] = df['datetime'].dt.date\n",
    "    df['is_holiday'] = df['date'].apply(lambda x: int(x in us_holidays))\n",
    "    \n",
    "    # Day before/after holiday\n",
    "    df['is_day_before_holiday'] = df['is_holiday'].shift(-24).fillna(0).astype(int)\n",
    "    df['is_day_after_holiday'] = df['is_holiday'].shift(24).fillna(0).astype(int)\n",
    "    \n",
    "    # Thanksgiving - Cooking load, midday peak\n",
    "    df['is_thanksgiving'] = df['date'].apply(\n",
    "        lambda x: int(1 if us_holidays.get(x) == 'Thanksgiving' else 0)\n",
    "    )\n",
    "    \n",
    "    # Christmas - Low commercial, high residential heating\n",
    "    df['is_christmas'] = df['date'].apply(\n",
    "        lambda x: int(1 if us_holidays.get(x) == 'Christmas Day' else 0)\n",
    "    )\n",
    "    \n",
    "    # New Year's Day - Late night/early morning shift\n",
    "    df['is_new_years'] = df['date'].apply(\n",
    "        lambda x: int(1 if us_holidays.get(x) == \"New Year's Day\" else 0)\n",
    "    )\n",
    "    \n",
    "    # July 4th - Summer, outdoor, evening grilling/fireworks\n",
    "    df['is_july4'] = df['date'].apply(\n",
    "        lambda x: int(1 if us_holidays.get(x) == 'Independence Day' else 0)\n",
    "    )\n",
    "    # Others\n",
    "    df['is_other_holiday'] = (df['is_holiday']-df['is_thanksgiving']-df['is_christmas']-df['is_new_years']-df['is_july4'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "df = add_features(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURE_COLS = [\n",
    "    # Temporal\n",
    "    'hour', 'day_of_week', 'month', 'day_of_month', 'day_of_year', 'week_of_year', 'is_weekend',\n",
    "    'hour_sin', 'hour_cos', 'day_sin', 'day_cos',\n",
    "    # Weather\n",
    "    'temperature', 'humidity', 'wind_speed', 'precipitation',\n",
    "    # 'is_holiday', 'is_day_before_holiday', 'is_day_after_holiday'\n",
    "    'is_thanksgiving', 'is_christmas', 'is_new_years', 'is_july4', 'is_other_holiday', 'is_day_before_holiday', 'is_day_after_holiday'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper functions defined\n"
     ]
    }
   ],
   "source": [
    "test_start = pd.to_datetime(TEST_START)\n",
    "test_end = pd.to_datetime(TEST_END)\n",
    "\n",
    "def prepare_data(df, test_start, test_end, FEATURE_COLS, region=None):\n",
    "    \"\"\"\n",
    "    Prepare train and test data for a given time period and region.\n",
    "    \"\"\"\n",
    "    # Filter by region if specified\n",
    "    if region is not None:\n",
    "        df = df[df['region'] == region].copy()\n",
    "    \n",
    "    # Convert string dates to datetime\n",
    "    test_start_dt = pd.Timestamp(test_start).tz_localize('America/New_York')\n",
    "    test_end_dt = pd.Timestamp(test_end).tz_localize('America/New_York')\n",
    "    \n",
    "    # Split data\n",
    "    test_data = df[(df['datetime'] >= test_start_dt) & (df['datetime'] <= test_end_dt)].copy()\n",
    "    \n",
    "    # Drop rows with missing lag features\n",
    "    test_data = test_data.dropna(subset=FEATURE_COLS)\n",
    "    \n",
    "    # Prepare X and y\n",
    "    X_test = test_data[FEATURE_COLS].values\n",
    "    y_test = test_data[TARGET_COL].values\n",
    "    \n",
    "    return X_test, y_test, test_data\n",
    "\n",
    "print(\"Helper functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of regions: 29\n",
      "Regions: ['AECO', 'AEPAPT', 'AEPIMP', 'AEPKPT', 'AEPOPT', 'AP', 'BC', 'CE', 'DAY', 'DEOK', 'DOM', 'DPLCO', 'DUQ', 'EASTON', 'EKPC', 'JC', 'ME', 'OE', 'OVEC', 'PAPWR', 'PE', 'PEPCO', 'PLCO', 'PN', 'PS', 'RECO', 'SMECO', 'UGI', 'VMEU']\n"
     ]
    }
   ],
   "source": [
    "regions = sorted(df['region'].unique())\n",
    "print(f\"Number of regions: {len(regions)}\")\n",
    "print(f\"Regions: {regions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Load the saved models\n",
    "models_dir = os.path.join(OUTPUT_DIR, 'trained_models')\n",
    "model_path = os.path.join(models_dir, 'hourly_load_models.pkl')\n",
    "\n",
    "print(\"Loading saved models...\")\n",
    "with open(model_path, 'rb') as f:\n",
    "    hourly_best_models = pickle.load(f)\n",
    "print(f\"Loaded {len(hourly_best_models)} regional models\")\n",
    "\n",
    "# Define prediction period\n",
    "PRED_START = '2025-11-20'  # Adjust as needed\n",
    "PRED_END = '2025-11-29'    # Adjust as needed\n",
    "\n",
    "print(f\"\\nGenerating predictions for {PRED_START} to {PRED_END}\")\n",
    "\n",
    "# Generate predictions for all regions\n",
    "predictions_list = []\n",
    "\n",
    "for region in tqdm(regions, desc=\"Generating predictions\"):\n",
    "    # Get model info for this region\n",
    "    model_info = hourly_best_models[region]\n",
    "    model = model_info['model']\n",
    "    feature_cols = model_info['feature_cols']\n",
    "    method = model_info['method']\n",
    "    \n",
    "    # Prepare prediction data\n",
    "    pred_data = df[df['load_area'] == region].copy()\n",
    "    pred_data = pred_data[(pred_data['datetime_beginning_ept'] >= PRED_START) & \n",
    "                          (pred_data['datetime_beginning_ept'] <= PRED_END)]\n",
    "    \n",
    "    if len(pred_data) == 0:\n",
    "        print(f\"Warning: No data for {region} in prediction period\")\n",
    "        continue\n",
    "    \n",
    "    # Get features\n",
    "    X_pred = pred_data[feature_cols]\n",
    "    \n",
    "    # Generate predictions\n",
    "    if method == '1. Linear + Interactions':\n",
    "        # Special handling for linear with interactions if needed\n",
    "        y_pred = model.predict(X_pred)\n",
    "    else:\n",
    "        y_pred = model.predict(X_pred)\n",
    "    \n",
    "    # Store predictions with metadata\n",
    "    pred_df = pred_data[['datetime_beginning_ept']].copy()\n",
    "    pred_df['region'] = region\n",
    "    pred_df['predicted_load'] = y_pred\n",
    "    pred_df['date'] = pd.to_datetime(pred_df['datetime_beginning_ept']).dt.date\n",
    "    pred_df['hour'] = pd.to_datetime(pred_df['datetime_beginning_ept']).dt.hour\n",
    "    \n",
    "    predictions_list.append(pred_df)\n",
    "\n",
    "# Combine all predictions\n",
    "all_predictions = pd.concat(predictions_list, ignore_index=True)\n",
    "\n",
    "print(f\"\\nTotal predictions generated: {len(all_predictions):,}\")\n",
    "\n",
    "# Create 3D structure: Pivot to get day x region x hour\n",
    "predictions_pivot = all_predictions.pivot_table(\n",
    "    index='date',\n",
    "    columns=['region', 'hour'],\n",
    "    values='predicted_load'\n",
    ")\n",
    "\n",
    "print(f\"\\nPredictions shape: {predictions_pivot.shape}\")\n",
    "print(f\"  Days: {len(predictions_pivot.index)}\")\n",
    "print(f\"  Regions: {len(regions)}\")\n",
    "print(f\"  Hours: 24\")\n",
    "\n",
    "# Alternative: Create a proper 3D array\n",
    "dates = sorted(all_predictions['date'].unique())\n",
    "hours = list(range(24))\n",
    "\n",
    "# Initialize 3D array: (days, regions, hours)\n",
    "predictions_3d = np.zeros((len(dates), len(regions), len(hours)))\n",
    "\n",
    "# Fill the 3D array\n",
    "for i, date in enumerate(dates):\n",
    "    for j, region in enumerate(regions):\n",
    "        for k, hour in enumerate(hours):\n",
    "            mask = (all_predictions['date'] == date) & \\\n",
    "                   (all_predictions['region'] == region) & \\\n",
    "                   (all_predictions['hour'] == hour)\n",
    "            \n",
    "            if mask.sum() > 0:\n",
    "                predictions_3d[i, j, k] = all_predictions.loc[mask, 'predicted_load'].values[0]\n",
    "            else:\n",
    "                predictions_3d[i, j, k] = np.nan\n",
    "\n",
    "print(f\"\\n3D Array shape: {predictions_3d.shape}\")\n",
    "print(f\"  Dimension 0 (days): {predictions_3d.shape[0]}\")\n",
    "print(f\"  Dimension 1 (regions): {predictions_3d.shape[1]}\")\n",
    "print(f\"  Dimension 2 (hours): {predictions_3d.shape[2]}\")\n",
    "\n",
    "# Create a more accessible DataFrame format\n",
    "print(\"\\nCreating multi-index DataFrame...\")\n",
    "predictions_df = all_predictions.pivot_table(\n",
    "    index=['date', 'region'],\n",
    "    columns='hour',\n",
    "    values='predicted_load'\n",
    ").reset_index()\n",
    "\n",
    "# Rename hour columns for clarity\n",
    "hour_cols = {i: f'hour_{i:02d}' for i in range(24)}\n",
    "predictions_df = predictions_df.rename(columns=hour_cols)\n",
    "\n",
    "print(f\"Predictions DataFrame shape: {predictions_df.shape}\")\n",
    "print(f\"Columns: {list(predictions_df.columns[:5])}...\")\n",
    "\n",
    "# Display sample\n",
    "print(\"\\nSample predictions:\")\n",
    "print(predictions_df.head(10))\n",
    "\n",
    "# Save predictions\n",
    "predictions_output = os.path.join(OUTPUT_DIR, 'predictions')\n",
    "os.makedirs(predictions_output, exist_ok=True)\n",
    "\n",
    "# Save as CSV\n",
    "csv_path = os.path.join(predictions_output, f'hourly_predictions_{PRED_START}_to_{PRED_END}.csv')\n",
    "predictions_df.to_csv(csv_path, index=False)\n",
    "print(f\"\\nPredictions saved to: {csv_path}\")\n",
    "\n",
    "# Save 3D array as numpy file\n",
    "array_path = os.path.join(predictions_output, f'predictions_3d_{PRED_START}_to_{PRED_END}.npy')\n",
    "np.save(array_path, predictions_3d)\n",
    "print(f\"3D array saved to: {array_path}\")\n",
    "\n",
    "# Save metadata\n",
    "metadata = {\n",
    "    'dates': dates,\n",
    "    'regions': regions,\n",
    "    'hours': hours,\n",
    "    'shape': predictions_3d.shape,\n",
    "    'period': f\"{PRED_START} to {PRED_END}\"\n",
    "}\n",
    "metadata_path = os.path.join(predictions_output, f'predictions_metadata_{PRED_START}_to_{PRED_END}.pkl')\n",
    "with open(metadata_path, 'wb') as f:\n",
    "    pickle.dump(metadata, f)\n",
    "print(f\"Metadata saved to: {metadata_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Prediction generation complete!\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
