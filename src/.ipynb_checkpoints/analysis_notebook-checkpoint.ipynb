{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comprehensive Analysis and Model Training\n",
    "\n",
    "This notebook performs:\n",
    "1. Data merging (load + weather)\n",
    "2. Seasonal analysis\n",
    "3. Best model finding for 3 tasks:\n",
    "   - Hourly load forecasting\n",
    "   - Peak hour prediction\n",
    "   - Peak days prediction\n",
    "4. Saves trained best models for production use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install xgboost lightgbm catboost holidays --break-system-packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import glob\n",
    "import warnings\n",
    "import holidays\n",
    "import pickle\n",
    "from datetime import datetime, timedelta\n",
    "import pytz\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from xgboost import XGBRegressor, XGBClassifier\n",
    "from lightgbm import LGBMRegressor, LGBMClassifier\n",
    "from catboost import CatBoostRegressor\n",
    "from tqdm import tqdm\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "# Directory paths (relative to src/)\n",
    "LOAD_DIR = \"../data/raw/hrl_load_metered_2016-2025\"\n",
    "WEATHER_DIR = \"../data/raw/weather\"\n",
    "PROCESSED_DIR = \"../data/processed\"\n",
    "FIGURES_DIR = \"../figures\"\n",
    "OUTPUT_DIR = \"../output\"\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(PROCESSED_DIR, exist_ok=True)\n",
    "os.makedirs(FIGURES_DIR, exist_ok=True)\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Target 29 load areas\n",
    "KEEP_AREAS = [\n",
    "    \"AECO\", \"AEPAPT\", \"AEPIMP\", \"AEPKPT\", \"AEPOPT\", \"AP\", \"BC\", \"CE\", \"DAY\", \"DEOK\",\n",
    "    \"DOM\", \"DPLCO\", \"DUQ\", \"EASTON\", \"EKPC\", \"JC\", \"ME\", \"OE\", \"OVEC\", \"PAPWR\",\n",
    "    \"PE\", \"PEPCO\", \"PLCO\", \"PN\", \"PS\", \"RECO\", \"SMECO\", \"UGI\", \"VMEU\"\n",
    "]\n",
    "\n",
    "# Best Model Finding dates (2023 test)\n",
    "TRAIN_START = '2016-01-01 00:00:00'\n",
    "TRAIN_END = '2023-10-30 23:00:00'\n",
    "TEST_START = '2023-11-20 00:00:00'\n",
    "TEST_END = '2023-11-29 23:00:00'\n",
    "\n",
    "# Rolling window for peak days\n",
    "WINDOW_SIZE = 10  # days\n",
    "NUM_PEAK_DAYS = 2\n",
    "\n",
    "print(\"Configuration loaded\")\n",
    "print(f\"\\nBest Model Finding Period:\")\n",
    "print(f\"  Train: {TRAIN_START} to {TRAIN_END}\")\n",
    "print(f\"  Test:  {TEST_START} to {TEST_END}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def n_distinct_na_omit(series):\n",
    "    \"\"\"Count unique non-null values in a series\"\"\"\n",
    "    return series.dropna().nunique()\n",
    "\n",
    "def add_seconds(time_str):\n",
    "    \"\"\"Add seconds to time string if not present\"\"\"\n",
    "    import re\n",
    "    if re.search(r':\\d{2}$', time_str):\n",
    "        return time_str + \":00\"\n",
    "    return time_str\n",
    "\n",
    "def parse_et(series):\n",
    "    \"\"\"Parse datetime with Eastern Time timezone handling\"\"\"\n",
    "    if pd.api.types.is_datetime64_any_dtype(series):\n",
    "        result = pd.to_datetime(series)\n",
    "        if result.dt.tz is None:\n",
    "            return result.dt.tz_localize('America/New_York', ambiguous='NaT', nonexistent='NaT')\n",
    "        return result\n",
    "    result = pd.to_datetime(series, format='%m/%d/%Y %I:%M:%S %p', errors='coerce')\n",
    "    mask = result.isna()\n",
    "    if mask.any():\n",
    "        result[mask] = pd.to_datetime(series[mask], errors='coerce')\n",
    "    return result.dt.tz_localize('America/New_York', ambiguous='NaT', nonexistent='NaT')\n",
    "\n",
    "print(\"Helper functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"STEP 1: LOADING AND MERGING DATA\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Load all yearly load files\n",
    "load_files = sorted(glob.glob(os.path.join(LOAD_DIR, \"hrl_load_metered_*.csv\")))\n",
    "\n",
    "if len(load_files) == 0:\n",
    "    raise ValueError(f\"No load files found in {LOAD_DIR}\")\n",
    "\n",
    "print(f\"\\nFound {len(load_files)} load files\")\n",
    "\n",
    "datasets = {}\n",
    "for f in load_files:\n",
    "    year = os.path.basename(f).split('_')[-1].replace('.csv', '')\n",
    "    df = pd.read_csv(f)\n",
    "    df['year'] = int(year)\n",
    "    datasets[year] = df\n",
    "    print(f\"  Loaded {year}: {len(df):,} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all years and filter to 29 areas\n",
    "load_df = pd.concat(datasets.values(), ignore_index=True)\n",
    "load_df = load_df[load_df['load_area'].isin(KEEP_AREAS)].copy()\n",
    "\n",
    "print(f\"\\nCombined load data: {len(load_df):,} rows\")\n",
    "print(f\"Unique load areas: {load_df['load_area'].nunique()}\")\n",
    "\n",
    "# Parse datetime\n",
    "load_df['datetime_beginning_ept'] = load_df['datetime_beginning_ept'].apply(add_seconds)\n",
    "load_df['dt'] = parse_et(load_df['datetime_beginning_ept'])\n",
    "load_df = load_df.dropna(subset=['dt'])\n",
    "\n",
    "print(f\"After datetime parsing: {len(load_df):,} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load weather data\n",
    "weather_files = sorted(glob.glob(os.path.join(WEATHER_DIR, \"*.csv\")))\n",
    "\n",
    "if len(weather_files) == 0:\n",
    "    raise ValueError(f\"No weather files found in {WEATHER_DIR}\")\n",
    "\n",
    "print(f\"\\nFound {len(weather_files)} weather files\")\n",
    "\n",
    "weather_dfs = []\n",
    "for f in weather_files:\n",
    "    area = os.path.basename(f).replace('_weather.csv', '')\n",
    "    if area in KEEP_AREAS:\n",
    "        df = pd.read_csv(f)\n",
    "        df['load_area'] = area\n",
    "        weather_dfs.append(df)\n",
    "        print(f\"  Loaded {area}: {len(df):,} rows\")\n",
    "\n",
    "weather_df = pd.concat(weather_dfs, ignore_index=True)\n",
    "\n",
    "# Parse weather datetime\n",
    "weather_df['datetime_beginning_ept'] = weather_df['datetime_beginning_ept'].apply(add_seconds)\n",
    "weather_df['dt'] = parse_et(weather_df['datetime_beginning_ept'])\n",
    "weather_df = weather_df.dropna(subset=['dt'])\n",
    "\n",
    "print(f\"\\nCombined weather data: {len(weather_df):,} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge load and weather data\n",
    "merged = pd.merge(\n",
    "    load_df[['load_area', 'dt', 'mw']],\n",
    "    weather_df[['load_area', 'dt', 'temp', 'humidity', 'precip', 'wind']],\n",
    "    on=['load_area', 'dt'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Rename datetime column\n",
    "merged = merged.rename(columns={'dt': 'datetime_beginning_ept'})\n",
    "\n",
    "# Sort by load_area and datetime\n",
    "merged = merged.sort_values(['load_area', 'datetime_beginning_ept']).reset_index(drop=True)\n",
    "\n",
    "print(f\"\\nMerged data: {len(merged):,} rows\")\n",
    "print(f\"Weather coverage: {(~merged['temp'].isna()).sum() / len(merged) * 100:.2f}%\")\n",
    "\n",
    "# Save merged data\n",
    "output_file = os.path.join(PROCESSED_DIR, \"merged_load_weather.csv\")\n",
    "merged.to_csv(output_file, index=False)\n",
    "print(f\"\\nMerged data saved to: {output_file}\")\n",
    "print(f\"File size: {os.path.getsize(output_file) / (1024**2):.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Seasonal Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"STEP 2: SEASONAL ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Prepare data for seasonal analysis\n",
    "df_season = merged.copy()\n",
    "df_season['datetime'] = pd.to_datetime(df_season['datetime_beginning_ept'])\n",
    "df_season['year'] = df_season['datetime'].dt.year\n",
    "df_season['month'] = df_season['datetime'].dt.month\n",
    "df_season['day'] = df_season['datetime'].dt.day\n",
    "df_season['hour'] = df_season['datetime'].dt.hour\n",
    "df_season['wday'] = df_season['datetime'].dt.dayofweek\n",
    "df_season['date'] = df_season['datetime'].dt.date\n",
    "\n",
    "# Define seasons\n",
    "def get_season(month):\n",
    "    if month in [3, 4, 5]:\n",
    "        return 'Spring'\n",
    "    elif month in [6, 7, 8]:\n",
    "        return 'Summer'\n",
    "    elif month in [9, 10, 11]:\n",
    "        return 'Fall'\n",
    "    else:\n",
    "        return 'Winter'\n",
    "\n",
    "df_season['season'] = df_season['month'].apply(get_season)\n",
    "\n",
    "# Day-hour index (0-167 for weekly cycle)\n",
    "df_season['dayhour'] = df_season['wday'] * 24 + df_season['hour']\n",
    "\n",
    "print(f\"\\nSeasonal data prepared: {len(df_season):,} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monthly and yearly aggregations\n",
    "monthly_tbl = df_season.groupby(['year', 'month']).agg({\n",
    "    'mw': 'mean',\n",
    "    'temp': 'mean',\n",
    "    'humidity': 'mean',\n",
    "    'wind': 'mean',\n",
    "    'precip': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "yearly_tbl = df_season.groupby('year').agg({\n",
    "    'mw': 'mean',\n",
    "    'temp': 'mean',\n",
    "    'humidity': 'mean',\n",
    "    'wind': 'mean',\n",
    "    'precip': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "print(f\"Monthly aggregations: {len(monthly_tbl)} rows\")\n",
    "print(f\"Yearly aggregations: {len(yearly_tbl)} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seasonal aggregations\n",
    "daily_tbl = df_season.groupby(['season', 'wday']).agg({\n",
    "    'mw': 'mean',\n",
    "    'temp': 'mean',\n",
    "    'humidity': 'mean',\n",
    "    'wind': 'mean',\n",
    "    'precip': 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "hourly_tbl = df_season.groupby(['season', 'hour']).agg({\n",
    "    'mw': 'mean',\n",
    "    'temp': 'mean',\n",
    "    'humidity': 'mean',\n",
    "    'wind': 'mean',\n",
    "    'precip': 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "dayhour_tbl = df_season.groupby(['season', 'dayhour']).agg({\n",
    "    'mw': 'mean',\n",
    "    'temp': 'mean',\n",
    "    'humidity': 'mean',\n",
    "    'wind': 'mean',\n",
    "    'precip': 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "print(f\"Daily patterns: {len(daily_tbl)} rows\")\n",
    "print(f\"Hourly patterns: {len(hourly_tbl)} rows\")\n",
    "print(f\"Day-hour patterns: {len(dayhour_tbl)} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 1: Monthly trends by year\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "\n",
    "for year in sorted(monthly_tbl['year'].unique()):\n",
    "    data = monthly_tbl[monthly_tbl['year'] == year]\n",
    "    axes[0, 0].plot(data['month'], data['mw'], marker='o', label=str(year), alpha=0.7)\n",
    "axes[0, 0].set_xlabel('Month')\n",
    "axes[0, 0].set_ylabel('Average Load (MW)')\n",
    "axes[0, 0].set_title('Monthly Load Trends by Year', fontweight='bold')\n",
    "axes[0, 0].legend(ncol=2, fontsize=8)\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "for year in sorted(monthly_tbl['year'].unique()):\n",
    "    data = monthly_tbl[monthly_tbl['year'] == year]\n",
    "    axes[0, 1].plot(data['month'], data['temp'], marker='o', label=str(year), alpha=0.7)\n",
    "axes[0, 1].set_xlabel('Month')\n",
    "axes[0, 1].set_ylabel('Average Temperature (°C)')\n",
    "axes[0, 1].set_title('Monthly Temperature Trends by Year', fontweight='bold')\n",
    "axes[0, 1].legend(ncol=2, fontsize=8)\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "for year in sorted(monthly_tbl['year'].unique()):\n",
    "    data = monthly_tbl[monthly_tbl['year'] == year]\n",
    "    axes[1, 0].plot(data['month'], data['humidity'], marker='o', label=str(year), alpha=0.7)\n",
    "axes[1, 0].set_xlabel('Month')\n",
    "axes[1, 0].set_ylabel('Average Humidity (%)')\n",
    "axes[1, 0].set_title('Monthly Humidity Trends by Year', fontweight='bold')\n",
    "axes[1, 0].legend(ncol=2, fontsize=8)\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "for year in sorted(monthly_tbl['year'].unique()):\n",
    "    data = monthly_tbl[monthly_tbl['year'] == year]\n",
    "    axes[1, 1].plot(data['month'], data['wind'], marker='o', label=str(year), alpha=0.7)\n",
    "axes[1, 1].set_xlabel('Month')\n",
    "axes[1, 1].set_ylabel('Average Wind Speed')\n",
    "axes[1, 1].set_title('Monthly Wind Speed Trends by Year', fontweight='bold')\n",
    "axes[1, 1].legend(ncol=2, fontsize=8)\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Monthly Trends Across Years', fontsize=16, fontweight='bold', y=0.995)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(FIGURES_DIR, 'monthly_trends.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nSaved: monthly_trends.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 2: Yearly aggregate trends\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "\n",
    "axes[0, 0].plot(yearly_tbl['year'], yearly_tbl['mw'], marker='o', linewidth=2, markersize=8, color='steelblue')\n",
    "axes[0, 0].set_xlabel('Year')\n",
    "axes[0, 0].set_ylabel('Average Load (MW)')\n",
    "axes[0, 0].set_title('Yearly Load Trend', fontweight='bold')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[0, 1].plot(yearly_tbl['year'], yearly_tbl['temp'], marker='o', linewidth=2, markersize=8, color='coral')\n",
    "axes[0, 1].set_xlabel('Year')\n",
    "axes[0, 1].set_ylabel('Average Temperature (°C)')\n",
    "axes[0, 1].set_title('Yearly Temperature Trend', fontweight='bold')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1, 0].plot(yearly_tbl['year'], yearly_tbl['humidity'], marker='o', linewidth=2, markersize=8, color='lightgreen')\n",
    "axes[1, 0].set_xlabel('Year')\n",
    "axes[1, 0].set_ylabel('Average Humidity (%)')\n",
    "axes[1, 0].set_title('Yearly Humidity Trend', fontweight='bold')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1, 1].plot(yearly_tbl['year'], yearly_tbl['wind'], marker='o', linewidth=2, markersize=8, color='purple')\n",
    "axes[1, 1].set_xlabel('Year')\n",
    "axes[1, 1].set_ylabel('Average Wind Speed')\n",
    "axes[1, 1].set_title('Yearly Wind Speed Trend', fontweight='bold')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Yearly Aggregate Trends', fontsize=16, fontweight='bold', y=0.995)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(FIGURES_DIR, 'yearly_trends.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Saved: yearly_trends.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot seasonal patterns by day of week\n",
    "def plot_seasonal_by_wday(season_name, data):\n",
    "    season_data = data[data['season'] == season_name]\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "    \n",
    "    wday_labels = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
    "    \n",
    "    axes[0, 0].plot(season_data['wday'], season_data['mw'], marker='o', linewidth=2, markersize=8)\n",
    "    axes[0, 0].set_xticks(range(7))\n",
    "    axes[0, 0].set_xticklabels(wday_labels)\n",
    "    axes[0, 0].set_ylabel('Average Load (MW)')\n",
    "    axes[0, 0].set_title(f'{season_name} - Load by Day of Week', fontweight='bold')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    axes[0, 1].plot(season_data['wday'], season_data['temp'], marker='o', linewidth=2, markersize=8, color='coral')\n",
    "    axes[0, 1].set_xticks(range(7))\n",
    "    axes[0, 1].set_xticklabels(wday_labels)\n",
    "    axes[0, 1].set_ylabel('Temperature (°C)')\n",
    "    axes[0, 1].set_title(f'{season_name} - Temperature by Day of Week', fontweight='bold')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    axes[1, 0].plot(season_data['wday'], season_data['humidity'], marker='o', linewidth=2, markersize=8, color='lightgreen')\n",
    "    axes[1, 0].set_xticks(range(7))\n",
    "    axes[1, 0].set_xticklabels(wday_labels)\n",
    "    axes[1, 0].set_ylabel('Humidity (%)')\n",
    "    axes[1, 0].set_title(f'{season_name} - Humidity by Day of Week', fontweight='bold')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    axes[1, 1].plot(season_data['wday'], season_data['wind'], marker='o', linewidth=2, markersize=8, color='purple')\n",
    "    axes[1, 1].set_xticks(range(7))\n",
    "    axes[1, 1].set_xticklabels(wday_labels)\n",
    "    axes[1, 1].set_ylabel('Wind Speed')\n",
    "    axes[1, 1].set_title(f'{season_name} - Wind Speed by Day of Week', fontweight='bold')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.suptitle(f'{season_name} - Patterns by Day of Week', fontsize=16, fontweight='bold', y=0.995)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(FIGURES_DIR, f'{season_name}_wday.png'), dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# Generate plots for all seasons\n",
    "print(\"\\nGenerating seasonal plots by day of week...\")\n",
    "for season in ['Spring', 'Summer', 'Fall', 'Winter']:\n",
    "    plot_seasonal_by_wday(season, daily_tbl)\n",
    "    print(f\"  Saved: {season}_wday.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot seasonal patterns by hour\n",
    "def plot_seasonal_by_hour(season_name, data):\n",
    "    season_data = data[data['season'] == season_name]\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "    \n",
    "    axes[0, 0].plot(season_data['hour'], season_data['mw'], marker='o', linewidth=2)\n",
    "    axes[0, 0].set_xlabel('Hour of Day')\n",
    "    axes[0, 0].set_ylabel('Average Load (MW)')\n",
    "    axes[0, 0].set_title(f'{season_name} - Load by Hour', fontweight='bold')\n",
    "    axes[0, 0].set_xticks(range(0, 24, 3))\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    axes[0, 1].plot(season_data['hour'], season_data['temp'], marker='o', linewidth=2, color='coral')\n",
    "    axes[0, 1].set_xlabel('Hour of Day')\n",
    "    axes[0, 1].set_ylabel('Temperature (°C)')\n",
    "    axes[0, 1].set_title(f'{season_name} - Temperature by Hour', fontweight='bold')\n",
    "    axes[0, 1].set_xticks(range(0, 24, 3))\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    axes[1, 0].plot(season_data['hour'], season_data['humidity'], marker='o', linewidth=2, color='lightgreen')\n",
    "    axes[1, 0].set_xlabel('Hour of Day')\n",
    "    axes[1, 0].set_ylabel('Humidity (%)')\n",
    "    axes[1, 0].set_title(f'{season_name} - Humidity by Hour', fontweight='bold')\n",
    "    axes[1, 0].set_xticks(range(0, 24, 3))\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    axes[1, 1].plot(season_data['hour'], season_data['wind'], marker='o', linewidth=2, color='purple')\n",
    "    axes[1, 1].set_xlabel('Hour of Day')\n",
    "    axes[1, 1].set_ylabel('Wind Speed')\n",
    "    axes[1, 1].set_title(f'{season_name} - Wind Speed by Hour', fontweight='bold')\n",
    "    axes[1, 1].set_xticks(range(0, 24, 3))\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.suptitle(f'{season_name} - Patterns by Hour of Day', fontsize=16, fontweight='bold', y=0.995)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(FIGURES_DIR, f'{season_name}_hour.png'), dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# Generate plots for all seasons\n",
    "print(\"\\nGenerating seasonal plots by hour...\")\n",
    "for season in ['Spring', 'Summer', 'Fall', 'Winter']:\n",
    "    plot_seasonal_by_hour(season, hourly_tbl)\n",
    "    print(f\"  Saved: {season}_hour.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Seasonal analysis complete!\")\n",
    "print(f\"Generated {len([f for f in os.listdir(FIGURES_DIR) if f.endswith('.png')])} plots\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prepare Data for Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"STEP 3: PREPARING DATA FOR MODEL TRAINING\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Load merged data\n",
    "df = pd.read_csv(os.path.join(PROCESSED_DIR, \"merged_load_weather.csv\"))\n",
    "\n",
    "# Parse datetime\n",
    "df['datetime'] = pd.to_datetime(df['datetime_beginning_ept'])\n",
    "df = df.dropna(subset=['datetime'])\n",
    "\n",
    "# Rename columns for consistency\n",
    "df = df.rename(columns={\n",
    "    'load_area': 'region',\n",
    "    'mw': 'load',\n",
    "    'temp': 'temperature',\n",
    "    'precip': 'precipitation',\n",
    "    'wind': 'wind_speed'\n",
    "})\n",
    "\n",
    "print(f\"\\nData loaded: {len(df):,} rows\")\n",
    "print(f\"Date range: {df['datetime'].min()} to {df['datetime'].max()}\")\n",
    "print(f\"Regions: {df['region'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering function\n",
    "def add_features(df):\n",
    "    \"\"\"Add temporal and calendar features\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Temporal features\n",
    "    df['year'] = df['datetime'].dt.year\n",
    "    df['month'] = df['datetime'].dt.month\n",
    "    df['day'] = df['datetime'].dt.day\n",
    "    df['hour'] = df['datetime'].dt.hour\n",
    "    df['dayofweek'] = df['datetime'].dt.dayofweek\n",
    "    df['dayofyear'] = df['datetime'].dt.dayofyear\n",
    "    df['weekofyear'] = df['datetime'].dt.isocalendar().week\n",
    "    df['quarter'] = df['datetime'].dt.quarter\n",
    "    df['date'] = df['datetime'].dt.date\n",
    "    \n",
    "    # Cyclical encoding\n",
    "    df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)\n",
    "    df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)\n",
    "    df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)\n",
    "    df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)\n",
    "    df['dayofweek_sin'] = np.sin(2 * np.pi * df['dayofweek'] / 7)\n",
    "    df['dayofweek_cos'] = np.cos(2 * np.pi * df['dayofweek'] / 7)\n",
    "    \n",
    "    # Calendar features\n",
    "    us_holidays = holidays.US(years=range(df['year'].min(), df['year'].max() + 1))\n",
    "    df['is_holiday'] = df['datetime'].dt.date.apply(lambda x: x in us_holidays).astype(int)\n",
    "    df['is_weekend'] = (df['dayofweek'] >= 5).astype(int)\n",
    "    \n",
    "    # Season\n",
    "    def get_season(month):\n",
    "        if month in [3, 4, 5]:\n",
    "            return 1  # Spring\n",
    "        elif month in [6, 7, 8]:\n",
    "            return 2  # Summer\n",
    "        elif month in [9, 10, 11]:\n",
    "            return 3  # Fall\n",
    "        else:\n",
    "            return 4  # Winter\n",
    "    \n",
    "    df['season'] = df['month'].apply(get_season)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Add features\n",
    "df = add_features(df)\n",
    "\n",
    "print(\"\\nFeature engineering complete\")\n",
    "print(f\"Total features: {len(df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add lagged features for load forecasting\n",
    "def add_lag_features(df, region):\n",
    "    \"\"\"Add lagged load features for a specific region\"\"\"\n",
    "    df_region = df[df['region'] == region].copy()\n",
    "    df_region = df_region.sort_values('datetime')\n",
    "    \n",
    "    # Lag features\n",
    "    for lag in [1, 2, 3, 24, 48, 168]:  # 1-3 hours, 1-2 days, 1 week\n",
    "        df_region[f'load_lag_{lag}'] = df_region['load'].shift(lag)\n",
    "    \n",
    "    # Rolling statistics\n",
    "    df_region['load_rolling_mean_24'] = df_region['load'].shift(1).rolling(24, min_periods=1).mean()\n",
    "    df_region['load_rolling_std_24'] = df_region['load'].shift(1).rolling(24, min_periods=1).std()\n",
    "    df_region['load_rolling_mean_168'] = df_region['load'].shift(1).rolling(168, min_periods=1).mean()\n",
    "    \n",
    "    return df_region\n",
    "\n",
    "print(\"Lag feature function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Task 1: Hourly Load Forecast - Best Model Finding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"TASK 1: HOURLY LOAD FORECAST - BEST MODEL FINDING\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Get list of regions\n",
    "regions = sorted(df['region'].unique())\n",
    "print(f\"\\nTraining models for {len(regions)} regions\")\n",
    "\n",
    "# Define training period\n",
    "train_start = pd.to_datetime(TRAIN_START)\n",
    "train_end = pd.to_datetime(TRAIN_END)\n",
    "test_start = pd.to_datetime(TEST_START)\n",
    "test_end = pd.to_datetime(TEST_END)\n",
    "\n",
    "print(f\"\\nTraining period: {train_start} to {train_end}\")\n",
    "print(f\"Testing period: {test_start} to {test_end}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define methods for hourly load\n",
    "def get_hourly_load_methods():\n",
    "    return {\n",
    "        '1. Linear + Interactions': None,  # Will create with PolynomialFeatures\n",
    "        '2. Random Forest': RandomForestRegressor(n_estimators=100, max_depth=20, random_state=42, n_jobs=-1),\n",
    "        '3. XGBoost': XGBRegressor(n_estimators=100, max_depth=6, learning_rate=0.1, random_state=42, n_jobs=-1),\n",
    "        '4. LightGBM': LGBMRegressor(n_estimators=100, max_depth=6, learning_rate=0.1, random_state=42, n_jobs=-1, verbose=-1),\n",
    "        '5. CatBoost': CatBoostRegressor(iterations=100, depth=6, learning_rate=0.1, random_state=42, verbose=0)\n",
    "    }\n",
    "\n",
    "print(\"Methods defined for hourly load forecasting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate hourly load models\n",
    "hourly_results = []\n",
    "hourly_best_models = {}  # Store best model per region\n",
    "\n",
    "feature_cols = ['temperature', 'humidity', 'precipitation', 'wind_speed',\n",
    "                'hour', 'dayofweek', 'month', 'year', 'dayofyear', 'weekofyear', 'quarter',\n",
    "                'hour_sin', 'hour_cos', 'month_sin', 'month_cos', 'dayofweek_sin', 'dayofweek_cos',\n",
    "                'is_holiday', 'is_weekend', 'season',\n",
    "                'load_lag_1', 'load_lag_2', 'load_lag_3', 'load_lag_24', 'load_lag_48', 'load_lag_168',\n",
    "                'load_rolling_mean_24', 'load_rolling_std_24', 'load_rolling_mean_168']\n",
    "\n",
    "print(\"\\nTraining hourly load models...\")\n",
    "print(f\"Features: {len(feature_cols)}\")\n",
    "\n",
    "for region in tqdm(regions, desc=\"Regions\"):\n",
    "    # Prepare data for this region\n",
    "    df_region = add_lag_features(df, region)\n",
    "    df_region = df_region.dropna(subset=feature_cols + ['load'])\n",
    "    \n",
    "    # Split train/test\n",
    "    train_mask = (df_region['datetime'] >= train_start) & (df_region['datetime'] <= train_end)\n",
    "    test_mask = (df_region['datetime'] >= test_start) & (df_region['datetime'] <= test_end)\n",
    "    \n",
    "    X_train = df_region.loc[train_mask, feature_cols]\n",
    "    y_train = df_region.loc[train_mask, 'load']\n",
    "    X_test = df_region.loc[test_mask, feature_cols]\n",
    "    y_test = df_region.loc[test_mask, 'load']\n",
    "    \n",
    "    if len(X_test) == 0:\n",
    "        continue\n",
    "    \n",
    "    # Train each method\n",
    "    methods = get_hourly_load_methods()\n",
    "    region_best_rmse = float('inf')\n",
    "    region_best_method = None\n",
    "    region_best_model = None\n",
    "    \n",
    "    for method_name, model in methods.items():\n",
    "        try:\n",
    "            if method_name == '1. Linear + Interactions':\n",
    "                # Create polynomial features for linear model\n",
    "                poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n",
    "                X_train_poly = poly.fit_transform(X_train)\n",
    "                X_test_poly = poly.transform(X_test)\n",
    "                \n",
    "                model = LinearRegression()\n",
    "                model.fit(X_train_poly, y_train)\n",
    "                y_pred = model.predict(X_test_poly)\n",
    "            else:\n",
    "                model.fit(X_train, y_train)\n",
    "                y_pred = model.predict(X_test)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            mse = mean_squared_error(y_test, y_pred)\n",
    "            rmse = np.sqrt(mse)\n",
    "            mae = mean_absolute_error(y_test, y_pred)\n",
    "            r2 = r2_score(y_test, y_pred)\n",
    "            \n",
    "            hourly_results.append({\n",
    "                'Region': region,\n",
    "                'Method': method_name,\n",
    "                'MSE': mse,\n",
    "                'RMSE': rmse,\n",
    "                'MAE': mae,\n",
    "                'R2': r2\n",
    "            })\n",
    "            \n",
    "            # Track best model for this region\n",
    "            if rmse < region_best_rmse:\n",
    "                region_best_rmse = rmse\n",
    "                region_best_method = method_name\n",
    "                region_best_model = model\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error with {method_name} for {region}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Store best model\n",
    "    if region_best_model is not None:\n",
    "        hourly_best_models[region] = {\n",
    "            'model': region_best_model,\n",
    "            'method': region_best_method,\n",
    "            'rmse': region_best_rmse,\n",
    "            'feature_cols': feature_cols\n",
    "        }\n",
    "\n",
    "hourly_results_df = pd.DataFrame(hourly_results)\n",
    "print(f\"\\nCompleted training for {len(regions)} regions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of hourly load results\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"HOURLY LOAD FORECAST - RESULTS SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Average performance by method\n",
    "summary = hourly_results_df.groupby('Method').agg({\n",
    "    'MSE': 'mean',\n",
    "    'RMSE': 'mean',\n",
    "    'MAE': 'mean',\n",
    "    'R2': 'mean'\n",
    "}).round(2).sort_values('RMSE')\n",
    "\n",
    "print(\"\\nAverage Performance Across All Regions:\")\n",
    "print(summary)\n",
    "\n",
    "# Best method overall\n",
    "best_method = summary.index[0]\n",
    "print(f\"\\nBest Overall Method: {best_method}\")\n",
    "print(f\"Average RMSE: {summary.loc[best_method, 'RMSE']:.2f} MW\")\n",
    "\n",
    "# Count wins per method\n",
    "best_per_region = hourly_results_df.loc[hourly_results_df.groupby('Region')['RMSE'].idxmin()]\n",
    "method_wins = best_per_region['Method'].value_counts()\n",
    "print(\"\\nMethod Wins by Region:\")\n",
    "print(method_wins)\n",
    "\n",
    "# Save results\n",
    "hourly_results_df.to_csv(os.path.join(OUTPUT_DIR, 'hourly_load_all_methods.csv'), index=False)\n",
    "summary.to_csv(os.path.join(OUTPUT_DIR, 'hourly_load_summary.csv'))\n",
    "best_per_region.to_csv(os.path.join(OUTPUT_DIR, 'hourly_load_best_per_region.csv'), index=False)\n",
    "\n",
    "print(\"\\nResults saved to output directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Task 2: Peak Hour Prediction - Best Model Finding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"TASK 2: PEAK HOUR PREDICTION - BEST MODEL FINDING\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Prepare daily-level data with peak hours\n",
    "df_daily = df.copy()\n",
    "\n",
    "# Calculate peak hour for each day and region\n",
    "peak_hours = df_daily.groupby(['region', 'date'])['load'].idxmax()\n",
    "df_daily['is_peak_hour'] = 0\n",
    "df_daily.loc[peak_hours, 'is_peak_hour'] = 1\n",
    "\n",
    "# Get the actual peak hour for each day-region\n",
    "peak_hour_actual = df_daily[df_daily['is_peak_hour'] == 1][['region', 'date', 'hour']]\n",
    "peak_hour_actual = peak_hour_actual.rename(columns={'hour': 'peak_hour'})\n",
    "\n",
    "print(f\"\\nPeak hour data prepared\")\n",
    "print(f\"Total day-region combinations: {len(peak_hour_actual)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define methods for peak hour\n",
    "def get_peak_hour_methods():\n",
    "    return {\n",
    "        '1. Linear + Interactions': 'regression',  # Will use regression to predict load\n",
    "        '2. Random Forest': RandomForestClassifier(n_estimators=100, max_depth=20, random_state=42, n_jobs=-1),\n",
    "        '3. XGBoost': XGBClassifier(n_estimators=100, max_depth=6, learning_rate=0.1, random_state=42, n_jobs=-1, eval_metric='logloss'),\n",
    "        '4. LightGBM': LGBMClassifier(n_estimators=100, max_depth=6, learning_rate=0.1, random_state=42, n_jobs=-1, verbose=-1),\n",
    "        '5. Historical Same Date': 'historical'  # Look at same calendar date in previous years\n",
    "    }\n",
    "\n",
    "print(\"Methods defined for peak hour prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate peak hour models\n",
    "peak_hour_results = []\n",
    "peak_hour_best_models = {}  # Store best model per region\n",
    "\n",
    "feature_cols_peak = ['temperature', 'humidity', 'precipitation', 'wind_speed',\n",
    "                     'hour', 'dayofweek', 'month', 'dayofyear',\n",
    "                     'hour_sin', 'hour_cos', 'dayofweek_sin', 'dayofweek_cos',\n",
    "                     'is_holiday', 'is_weekend', 'season']\n",
    "\n",
    "print(\"\\nTraining peak hour models...\")\n",
    "print(f\"Features: {len(feature_cols_peak)}\")\n",
    "\n",
    "for region in tqdm(regions, desc=\"Regions\"):\n",
    "    # Prepare data for this region\n",
    "    df_region = df_daily[df_daily['region'] == region].copy()\n",
    "    df_region = df_region.dropna(subset=feature_cols_peak)\n",
    "    \n",
    "    # Split train/test\n",
    "    train_mask = (df_region['datetime'] >= train_start) & (df_region['datetime'] <= train_end)\n",
    "    test_mask = (df_region['datetime'] >= test_start) & (df_region['datetime'] <= test_end)\n",
    "    \n",
    "    X_train = df_region.loc[train_mask, feature_cols_peak]\n",
    "    y_train = df_region.loc[train_mask, 'hour']\n",
    "    X_test = df_region.loc[test_mask, feature_cols_peak]\n",
    "    y_test = df_region.loc[test_mask, 'hour']\n",
    "    \n",
    "    # Get test dates for daily aggregation\n",
    "    test_dates = df_region.loc[test_mask, 'date'].unique()\n",
    "    \n",
    "    if len(test_dates) == 0:\n",
    "        continue\n",
    "    \n",
    "    # Get actual peak hours for test dates\n",
    "    actual_peaks = peak_hour_actual[\n",
    "        (peak_hour_actual['region'] == region) &\n",
    "        (peak_hour_actual['date'].isin(test_dates))\n",
    "    ]\n",
    "    \n",
    "    # Train each method\n",
    "    methods = get_peak_hour_methods()\n",
    "    region_best_success = 0\n",
    "    region_best_method = None\n",
    "    region_best_model = None\n",
    "    \n",
    "    for method_name, model in methods.items():\n",
    "        try:\n",
    "            predictions = []\n",
    "            \n",
    "            if method_name == '1. Linear + Interactions':\n",
    "                # Predict load for each hour, pick max\n",
    "                lr_model = LinearRegression()\n",
    "                lr_train_y = df_region.loc[train_mask, 'load']\n",
    "                lr_model.fit(X_train, lr_train_y)\n",
    "                \n",
    "                for test_date in test_dates:\n",
    "                    date_mask = df_region['date'] == test_date\n",
    "                    X_date = df_region.loc[date_mask & test_mask, feature_cols_peak]\n",
    "                    pred_loads = lr_model.predict(X_date)\n",
    "                    pred_hour = df_region.loc[date_mask & test_mask, 'hour'].iloc[pred_loads.argmax()]\n",
    "                    predictions.append(pred_hour)\n",
    "                \n",
    "            elif method_name == '5. Historical Same Date':\n",
    "                # Use same date from previous year\n",
    "                for test_date in test_dates:\n",
    "                    prev_years_data = df_region[\n",
    "                        (df_region['month'] == test_date.month) &\n",
    "                        (df_region['day'] == test_date.day) &\n",
    "                        (df_region['datetime'] < test_start)\n",
    "                    ]\n",
    "                    if len(prev_years_data) > 0:\n",
    "                        # Get most common peak hour for this date\n",
    "                        peak_data = prev_years_data.groupby('date')['load'].idxmax()\n",
    "                        peak_hours_hist = prev_years_data.loc[peak_data, 'hour']\n",
    "                        pred_hour = peak_hours_hist.mode()[0] if len(peak_hours_hist.mode()) > 0 else 17\n",
    "                    else:\n",
    "                        pred_hour = 17  # Default\n",
    "                    predictions.append(pred_hour)\n",
    "                \n",
    "            else:\n",
    "                # Classification models\n",
    "                model.fit(X_train, y_train)\n",
    "                \n",
    "                for test_date in test_dates:\n",
    "                    date_mask = df_region['date'] == test_date\n",
    "                    X_date = df_region.loc[date_mask & test_mask, feature_cols_peak]\n",
    "                    \n",
    "                    if hasattr(model, 'predict_proba'):\n",
    "                        probs = model.predict_proba(X_date)\n",
    "                        pred_hour = model.classes_[probs.sum(axis=0).argmax()]\n",
    "                    else:\n",
    "                        preds = model.predict(X_date)\n",
    "                        pred_hour = df_region.loc[date_mask & test_mask, 'hour'].iloc[0]\n",
    "                    \n",
    "                    predictions.append(pred_hour)\n",
    "            \n",
    "            # Calculate success rate (correct within ±1 hour)\n",
    "            predictions = np.array(predictions)\n",
    "            actuals = actual_peaks['peak_hour'].values\n",
    "            \n",
    "            correct = np.abs(predictions - actuals) <= 1\n",
    "            success_rate = correct.sum() / len(correct) * 100\n",
    "            \n",
    "            peak_hour_results.append({\n",
    "                'Region': region,\n",
    "                'Method': method_name,\n",
    "                'Total': len(predictions),\n",
    "                'Correct': correct.sum(),\n",
    "                'Incorrect': len(correct) - correct.sum(),\n",
    "                'Success_Rate_%': success_rate\n",
    "            })\n",
    "            \n",
    "            # Track best model for this region\n",
    "            if success_rate > region_best_success:\n",
    "                region_best_success = success_rate\n",
    "                region_best_method = method_name\n",
    "                region_best_model = model if method_name not in ['1. Linear + Interactions', '5. Historical Same Date'] else method_name\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error with {method_name} for {region}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Store best model\n",
    "    if region_best_model is not None:\n",
    "        peak_hour_best_models[region] = {\n",
    "            'model': region_best_model,\n",
    "            'method': region_best_method,\n",
    "            'success_rate': region_best_success,\n",
    "            'feature_cols': feature_cols_peak\n",
    "        }\n",
    "\n",
    "peak_hour_results_df = pd.DataFrame(peak_hour_results)\n",
    "print(f\"\\nCompleted training for {len(regions)} regions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of peak hour results\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"PEAK HOUR PREDICTION - RESULTS SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Aggregate by method\n",
    "summary = peak_hour_results_df.groupby('Method').agg({\n",
    "    'Total': 'sum',\n",
    "    'Correct': 'sum',\n",
    "    'Incorrect': 'sum'\n",
    "}).reset_index()\n",
    "summary['Success_Rate_%'] = (summary['Correct'] / summary['Total'] * 100).round(2)\n",
    "summary = summary.sort_values('Success_Rate_%', ascending=False)\n",
    "\n",
    "print(\"\\nOverall Performance:\")\n",
    "print(summary.to_string(index=False))\n",
    "\n",
    "# Best method\n",
    "best_method = summary.iloc[0]['Method']\n",
    "best_success = summary.iloc[0]['Success_Rate_%']\n",
    "print(f\"\\nBest Method: {best_method} ({best_success:.2f}%)\")\n",
    "\n",
    "# Save results\n",
    "peak_hour_results_df.to_csv(os.path.join(OUTPUT_DIR, 'peak_hour_all_methods.csv'), index=False)\n",
    "summary.to_csv(os.path.join(OUTPUT_DIR, 'peak_hour_summary.csv'), index=False)\n",
    "\n",
    "print(\"\\nResults saved to output directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Task 3: Peak Days Prediction - Best Model Finding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"TASK 3: PEAK DAYS PREDICTION - BEST MODEL FINDING\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Create training data with rolling windows\n",
    "def create_peak_days_windows(df, region, start_date, end_date):\n",
    "    \"\"\"Create rolling 10-day windows with peak day labels\"\"\"\n",
    "    df_region = df[df['region'] == region].copy()\n",
    "    df_region = df_region.sort_values('datetime')\n",
    "    \n",
    "    # Filter date range\n",
    "    df_region = df_region[\n",
    "        (df_region['datetime'] >= pd.to_datetime(start_date)) &\n",
    "        (df_region['datetime'] <= pd.to_datetime(end_date))\n",
    "    ]\n",
    "    \n",
    "    # Group by date and get daily max load\n",
    "    daily = df_region.groupby('date').agg({\n",
    "        'load': 'max',\n",
    "        'temperature': 'mean',\n",
    "        'humidity': 'mean',\n",
    "        'precipitation': 'sum',\n",
    "        'wind_speed': 'mean',\n",
    "        'month': 'first',\n",
    "        'dayofweek': 'first',\n",
    "        'dayofyear': 'first',\n",
    "        'is_holiday': 'first',\n",
    "        'is_weekend': 'first',\n",
    "        'season': 'first'\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Create rolling windows\n",
    "    windows = []\n",
    "    dates = sorted(daily['date'].unique())\n",
    "    \n",
    "    for i in range(len(dates) - WINDOW_SIZE + 1):\n",
    "        window_dates = dates[i:i+WINDOW_SIZE]\n",
    "        window_data = daily[daily['date'].isin(window_dates)].copy()\n",
    "        \n",
    "        # Mark top 2 days as peak days\n",
    "        top_days = window_data.nlargest(NUM_PEAK_DAYS, 'load')['date'].values\n",
    "        window_data['is_peak_day'] = window_data['date'].isin(top_days).astype(int)\n",
    "        \n",
    "        windows.append(window_data)\n",
    "    \n",
    "    if len(windows) > 0:\n",
    "        return pd.concat(windows, ignore_index=True)\n",
    "    return pd.DataFrame()\n",
    "\n",
    "print(f\"\\nWindow size: {WINDOW_SIZE} days\")\n",
    "print(f\"Peak days per window: {NUM_PEAK_DAYS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define methods for peak days (NO LOAD FEATURES)\n",
    "def get_peak_days_methods():\n",
    "    return {\n",
    "        '1. RF Regression + Ranking': RandomForestRegressor(n_estimators=100, max_depth=20, random_state=42, n_jobs=-1),\n",
    "        '2. RF Classification + Asymmetric Loss': RandomForestClassifier(\n",
    "            n_estimators=100, max_depth=20, random_state=42, n_jobs=-1, class_weight={0: 1, 1: 4}\n",
    "        ),\n",
    "        '3. XGBoost Classification + Asymmetric Loss': XGBClassifier(\n",
    "            n_estimators=100, max_depth=6, learning_rate=0.1, \n",
    "            random_state=42, n_jobs=-1, scale_pos_weight=4, eval_metric='logloss'\n",
    "        )\n",
    "    }\n",
    "\n",
    "# Features: NO LOAD FEATURES\n",
    "feature_cols_peak_days = ['temperature', 'humidity', 'precipitation', 'wind_speed',\n",
    "                          'month', 'dayofweek', 'dayofyear',\n",
    "                          'is_holiday', 'is_weekend', 'season']\n",
    "\n",
    "print(\"Methods defined for peak days prediction\")\n",
    "print(f\"Features (NO LOAD): {len(feature_cols_peak_days)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate peak days models\n",
    "peak_days_results = []\n",
    "peak_days_best_models = {}  # Store best model per region\n",
    "\n",
    "print(\"\\nTraining peak days models...\")\n",
    "\n",
    "for region in tqdm(regions, desc=\"Regions\"):\n",
    "    # Create training windows\n",
    "    train_windows = create_peak_days_windows(df, region, TRAIN_START, TRAIN_END)\n",
    "    \n",
    "    if len(train_windows) == 0:\n",
    "        continue\n",
    "    \n",
    "    # Prepare test data (single 10-day window)\n",
    "    test_data = df[\n",
    "        (df['region'] == region) &\n",
    "        (df['datetime'] >= test_start) &\n",
    "        (df['datetime'] <= test_end)\n",
    "    ].copy()\n",
    "    \n",
    "    test_daily = test_data.groupby('date').agg({\n",
    "        'load': 'max',\n",
    "        'temperature': 'mean',\n",
    "        'humidity': 'mean',\n",
    "        'precipitation': 'sum',\n",
    "        'wind_speed': 'mean',\n",
    "        'month': 'first',\n",
    "        'dayofweek': 'first',\n",
    "        'dayofyear': 'first',\n",
    "        'is_holiday': 'first',\n",
    "        'is_weekend': 'first',\n",
    "        'season': 'first'\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Mark actual peak days\n",
    "    actual_peak_days = test_daily.nlargest(NUM_PEAK_DAYS, 'load')['date'].values\n",
    "    test_daily['is_peak_day'] = test_daily['date'].isin(actual_peak_days).astype(int)\n",
    "    \n",
    "    if len(test_daily) != 10:\n",
    "        continue\n",
    "    \n",
    "    # Train each method\n",
    "    X_train = train_windows[feature_cols_peak_days]\n",
    "    y_train = train_windows['is_peak_day']\n",
    "    X_test = test_daily[feature_cols_peak_days]\n",
    "    y_test = test_daily['is_peak_day']\n",
    "    \n",
    "    methods = get_peak_days_methods()\n",
    "    region_best_loss = float('inf')\n",
    "    region_best_method = None\n",
    "    region_best_model = None\n",
    "    \n",
    "    for method_name, model in methods.items():\n",
    "        try:\n",
    "            if '1. RF Regression' in method_name:\n",
    "                # Regression: predict load, rank, select top 2\n",
    "                # Train to predict peak hour load\n",
    "                y_train_load = train_windows['load']\n",
    "                model.fit(X_train, y_train_load)\n",
    "                pred_loads = model.predict(X_test)\n",
    "                \n",
    "                # Select top 2 days\n",
    "                top_2_indices = pred_loads.argsort()[-2:]\n",
    "                y_pred = np.zeros(len(y_test))\n",
    "                y_pred[top_2_indices] = 1\n",
    "            else:\n",
    "                # Classification: predict probability, select top 2\n",
    "                model.fit(X_train, y_train)\n",
    "                pred_probs = model.predict_proba(X_test)[:, 1]\n",
    "                \n",
    "                # Select top 2 days by probability\n",
    "                top_2_indices = pred_probs.argsort()[-2:]\n",
    "                y_pred = np.zeros(len(y_test))\n",
    "                y_pred[top_2_indices] = 1\n",
    "            \n",
    "            # Calculate loss (FN=4, FP=1)\n",
    "            fn = ((y_test == 1) & (y_pred == 0)).sum() * 4\n",
    "            fp = ((y_test == 0) & (y_pred == 1)).sum() * 1\n",
    "            total_loss = fn + fp\n",
    "            \n",
    "            # Other metrics\n",
    "            tp = ((y_test == 1) & (y_pred == 1)).sum()\n",
    "            tn = ((y_test == 0) & (y_pred == 0)).sum()\n",
    "            precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "            recall = tp / (tp + fn/4) if (tp + fn/4) > 0 else 0  # fn/4 to get actual count\n",
    "            \n",
    "            peak_days_results.append({\n",
    "                'Region': region,\n",
    "                'Method': method_name,\n",
    "                'loss': total_loss,\n",
    "                'fn': int(fn / 4),  # Actual count\n",
    "                'fp': int(fp),\n",
    "                'recall': recall,\n",
    "                'precision': precision\n",
    "            })\n",
    "            \n",
    "            # Track best model for this region\n",
    "            if total_loss < region_best_loss:\n",
    "                region_best_loss = total_loss\n",
    "                region_best_method = method_name\n",
    "                region_best_model = model\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error with {method_name} for {region}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Store best model\n",
    "    if region_best_model is not None:\n",
    "        peak_days_best_models[region] = {\n",
    "            'model': region_best_model,\n",
    "            'method': region_best_method,\n",
    "            'loss': region_best_loss,\n",
    "            'feature_cols': feature_cols_peak_days\n",
    "        }\n",
    "\n",
    "peak_days_results_df = pd.DataFrame(peak_days_results)\n",
    "print(f\"\\nCompleted training for {len(regions)} regions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of peak days results\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"PEAK DAYS PREDICTION - RESULTS SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Aggregate by method\n",
    "summary = peak_days_results_df.groupby('Method').agg({\n",
    "    'loss': 'sum',\n",
    "    'fn': 'sum',\n",
    "    'fp': 'sum',\n",
    "    'recall': 'mean',\n",
    "    'precision': 'mean'\n",
    "}).reset_index()\n",
    "summary = summary.sort_values('loss')\n",
    "\n",
    "print(\"\\nOverall Performance (NO LOAD FEATURES):\")\n",
    "print(summary.to_string(index=False))\n",
    "\n",
    "# Best method\n",
    "best_method = summary.iloc[0]['Method']\n",
    "best_loss = summary.iloc[0]['loss']\n",
    "print(f\"\\nBest Method: {best_method} (Loss: {best_loss})\")\n",
    "\n",
    "# Save results\n",
    "peak_days_results_df.to_csv(os.path.join(OUTPUT_DIR, 'peak_days_all_methods.csv'), index=False)\n",
    "summary.to_csv(os.path.join(OUTPUT_DIR, 'peak_days_summary.csv'), index=False)\n",
    "\n",
    "print(\"\\nResults saved to output directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Trained Best Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"SAVING TRAINED BEST MODELS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Create models directory\n",
    "models_dir = os.path.join(OUTPUT_DIR, 'trained_models')\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "# Save hourly load models\n",
    "print(\"\\nSaving hourly load forecast models...\")\n",
    "with open(os.path.join(models_dir, 'hourly_load_models.pkl'), 'wb') as f:\n",
    "    pickle.dump(hourly_best_models, f)\n",
    "print(f\"  Saved {len(hourly_best_models)} regional models\")\n",
    "\n",
    "# Save peak hour models\n",
    "print(\"\\nSaving peak hour prediction models...\")\n",
    "with open(os.path.join(models_dir, 'peak_hour_models.pkl'), 'wb') as f:\n",
    "    pickle.dump(peak_hour_best_models, f)\n",
    "print(f\"  Saved {len(peak_hour_best_models)} regional models\")\n",
    "\n",
    "# Save peak days models\n",
    "print(\"\\nSaving peak days prediction models...\")\n",
    "with open(os.path.join(models_dir, 'peak_days_models.pkl'), 'wb') as f:\n",
    "    pickle.dump(peak_days_best_models, f)\n",
    "print(f\"  Saved {len(peak_days_best_models)} regional models\")\n",
    "\n",
    "print(f\"\\nAll models saved to: {models_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"COMPREHENSIVE ANALYSIS COMPLETE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\n1. DATA MERGING\")\n",
    "print(f\"   - Merged data saved: {os.path.join(PROCESSED_DIR, 'merged_load_weather.csv')}\")\n",
    "print(f\"   - Total rows: {len(merged):,}\")\n",
    "\n",
    "print(\"\\n2. SEASONAL ANALYSIS\")\n",
    "print(f\"   - Plots saved: {FIGURES_DIR}/\")\n",
    "print(f\"   - Total figures: {len([f for f in os.listdir(FIGURES_DIR) if f.endswith('.png')])}\")\n",
    "\n",
    "print(\"\\n3. BEST MODEL FINDING\")\n",
    "print(\"   Task 1: Hourly Load Forecast\")\n",
    "print(f\"     - Best overall method: {best_method}\")\n",
    "print(f\"     - Trained models: {len(hourly_best_models)} regions\")\n",
    "\n",
    "print(\"   Task 2: Peak Hour Prediction\")\n",
    "best_peak_hour = peak_hour_results_df.groupby('Method')['Success_Rate_%'].mean().idxmax()\n",
    "print(f\"     - Best overall method: {best_peak_hour}\")\n",
    "print(f\"     - Trained models: {len(peak_hour_best_models)} regions\")\n",
    "\n",
    "print(\"   Task 3: Peak Days Prediction\")\n",
    "best_peak_days = peak_days_results_df.groupby('Method')['loss'].sum().idxmin()\n",
    "print(f\"     - Best overall method: {best_peak_days}\")\n",
    "print(f\"     - Trained models: {len(peak_days_best_models)} regions\")\n",
    "\n",
    "print(\"\\n4. SAVED OUTPUTS\")\n",
    "print(f\"   - Results CSVs: {OUTPUT_DIR}/\")\n",
    "print(f\"   - Trained models: {os.path.join(OUTPUT_DIR, 'trained_models')}/\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"All tasks completed successfully!\")\n",
    "print(\"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
