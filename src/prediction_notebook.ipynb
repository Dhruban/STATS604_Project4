{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weather Forecast Data Download (Nov 20-29, 2025)\n",
    "\n",
    "This notebook downloads hourly weather **forecast** data from Open-Meteo API for multiple load areas across the PJM region.\n",
    "Data is retrieved in UTC and converted to Eastern Time (ET).\n",
    "\n",
    "**Date Range**: November 20, 2025 00:00 UTC to November 29, 2025 23:00 UTC  \n",
    "**Note**: Uses forecast API for future dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import pytz\n",
    "import os\n",
    "import time\n",
    "import holidays\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import glob\n",
    "import warnings\n",
    "import holidays\n",
    "import pickle\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from xgboost import XGBRegressor, XGBClassifier\n",
    "from lightgbm import LGBMRegressor, LGBMClassifier\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from tqdm import tqdm\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define Load Area Coordinates\n",
    "\n",
    "Define the latitude and longitude for each load area in the PJM region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total load areas: 29\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>load_area</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AECO</td>\n",
       "      <td>39.45</td>\n",
       "      <td>-74.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AEPAPT</td>\n",
       "      <td>37.25</td>\n",
       "      <td>-81.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AEPIMP</td>\n",
       "      <td>38.45</td>\n",
       "      <td>-81.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AEPKPT</td>\n",
       "      <td>38.20</td>\n",
       "      <td>-83.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AEPOPT</td>\n",
       "      <td>39.90</td>\n",
       "      <td>-82.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  load_area    lat   lon\n",
       "0      AECO  39.45 -74.5\n",
       "1    AEPAPT  37.25 -81.3\n",
       "2    AEPIMP  38.45 -81.6\n",
       "3    AEPKPT  38.20 -83.1\n",
       "4    AEPOPT  39.90 -82.9"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zone_coords = pd.DataFrame({\n",
    "    'load_area': ['AECO', 'AEPAPT', 'AEPIMP', 'AEPKPT', 'AEPOPT', 'AP', 'BC', 'CE', 'DAY', 'DEOK',\n",
    "                  'DOM', 'DPLCO', 'DUQ', 'EASTON', 'EKPC', 'JC', 'ME', 'OE', 'OVEC', 'PAPWR',\n",
    "                  'PE', 'PEPCO', 'PLCO', 'PN', 'PS', 'RECO', 'SMECO', 'UGI', 'VMEU'],\n",
    "    'lat': [39.45, 37.25, 38.45, 38.20, 39.90, 37.30, 40.80, 41.85, 39.75, 39.10,\n",
    "            37.55, 38.90, 40.45, 39.55, 37.75, 40.35, 40.20, 41.10, 38.85, 40.70,\n",
    "            40.00, 38.90, 40.95, 41.15, 40.75, 41.00, 38.40, 40.25, 37.30],\n",
    "    'lon': [-74.50, -81.30, -81.60, -83.10, -82.90, -80.90, -79.95, -86.10, -84.20, -84.50,\n",
    "            -77.45, -75.50, -79.90, -75.10, -84.30, -74.65, -76.00, -81.25, -82.85, -77.80,\n",
    "            -75.20, -76.95, -77.40, -77.80, -74.15, -74.10, -76.70, -75.65, -76.00]\n",
    "})\n",
    "\n",
    "print(f\"Total load areas: {len(zone_coords)}\")\n",
    "zone_coords.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configure API Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date range: 2025-11-20 to 2025-11-30\n",
      "Variables: temperature_2m,relative_humidity_2m,dew_point_2m,precipitation,wind_speed_10m\n",
      "Using forecast API for future dates\n"
     ]
    }
   ],
   "source": [
    "# Date range to download\n",
    "start_date = \"2025-11-20\"\n",
    "end_date = \"2025-11-30\"\n",
    "\n",
    "# API endpoint - using forecast API for future dates\n",
    "api = \"https://api.open-meteo.com/v1/forecast\"\n",
    "\n",
    "# Weather variables to retrieve\n",
    "hourly_vars = \"temperature_2m,relative_humidity_2m,dew_point_2m,precipitation,wind_speed_10m\"\n",
    "\n",
    "print(f\"Date range: {start_date} to {end_date}\")\n",
    "print(f\"Variables: {hourly_vars}\")\n",
    "print(f\"Using forecast API for future dates\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define Weather Data Fetcher Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_weather_data(lat, lon, start_date, end_date, tries=3):\n",
    "    \"\"\"Fetch weather data for a specific date range and location\"\"\"\n",
    "    # For forecast API, we use forecast_days parameter (max 16 days)\n",
    "    url = (f\"{api}?latitude={lat}&longitude={lon}\"\n",
    "           f\"&hourly={hourly_vars}&timezone=UTC&forecast_days=16\")\n",
    "    \n",
    "    for k in range(1, tries + 1):\n",
    "        try:\n",
    "            res = requests.get(url, timeout=60)\n",
    "            if res.status_code == 200:\n",
    "                j = res.json()\n",
    "                if 'hourly' in j and 'time' in j['hourly']:\n",
    "                    df = pd.DataFrame({\n",
    "                        'datetime_beginning_utc': j['hourly']['time'],\n",
    "                        'temp': j['hourly']['temperature_2m'],\n",
    "                        'humidity': j['hourly']['relative_humidity_2m'],\n",
    "                        'dew_point': j['hourly']['dew_point_2m'],\n",
    "                        'precip': j['hourly']['precipitation'],\n",
    "                        'wind': j['hourly']['wind_speed_10m']\n",
    "                    })\n",
    "                    \n",
    "                    # Filter to the specific date range we want\n",
    "                    df['datetime_beginning_utc'] = pd.to_datetime(df['datetime_beginning_utc'])\n",
    "                    mask = (df['datetime_beginning_utc'] >= start_date) & (df['datetime_beginning_utc'] <= end_date + ' 23:00:00')\n",
    "                    df = df[mask].copy()\n",
    "                    df['datetime_beginning_utc'] = df['datetime_beginning_utc'].astype(str)\n",
    "                    \n",
    "                    return df\n",
    "                else:\n",
    "                    print(f\"Unexpected response format for lat={lat}, lon={lon}\")\n",
    "            else:\n",
    "                print(f\"HTTP {res.status_code} for lat={lat}, lon={lon}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error on attempt {k} for lat={lat}, lon={lon}: {e}\")\n",
    "        \n",
    "        if k < tries:\n",
    "            time.sleep(0.7 * k)\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Download Weather Data for All Load Areas\n",
    "\n",
    "This cell downloads weather data for the specified date range across all load areas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading weather data for 29 load areas...\n",
      "\n",
      "============================================================\n",
      "[1/29] Fetching AECO (lat=39.45, lon=-74.5)...\n",
      "  ✓ Successfully fetched 240 hourly records\n",
      "[2/29] Fetching AEPAPT (lat=37.25, lon=-81.3)...\n",
      "  ✓ Successfully fetched 240 hourly records\n",
      "[3/29] Fetching AEPIMP (lat=38.45, lon=-81.6)...\n",
      "  ✓ Successfully fetched 240 hourly records\n",
      "[4/29] Fetching AEPKPT (lat=38.2, lon=-83.1)...\n",
      "  ✓ Successfully fetched 240 hourly records\n",
      "[5/29] Fetching AEPOPT (lat=39.9, lon=-82.9)...\n",
      "  ✓ Successfully fetched 240 hourly records\n",
      "[6/29] Fetching AP (lat=37.3, lon=-80.9)...\n",
      "  ✓ Successfully fetched 240 hourly records\n",
      "[7/29] Fetching BC (lat=40.8, lon=-79.95)...\n",
      "  ✓ Successfully fetched 240 hourly records\n",
      "[8/29] Fetching CE (lat=41.85, lon=-86.1)...\n",
      "  ✓ Successfully fetched 240 hourly records\n",
      "[9/29] Fetching DAY (lat=39.75, lon=-84.2)...\n",
      "  ✓ Successfully fetched 240 hourly records\n",
      "[10/29] Fetching DEOK (lat=39.1, lon=-84.5)...\n",
      "  ✓ Successfully fetched 240 hourly records\n",
      "[11/29] Fetching DOM (lat=37.55, lon=-77.45)...\n",
      "  ✓ Successfully fetched 240 hourly records\n",
      "[12/29] Fetching DPLCO (lat=38.9, lon=-75.5)...\n",
      "  ✓ Successfully fetched 240 hourly records\n",
      "[13/29] Fetching DUQ (lat=40.45, lon=-79.9)...\n",
      "  ✓ Successfully fetched 240 hourly records\n",
      "[14/29] Fetching EASTON (lat=39.55, lon=-75.1)...\n",
      "  ✓ Successfully fetched 240 hourly records\n",
      "[15/29] Fetching EKPC (lat=37.75, lon=-84.3)...\n",
      "  ✓ Successfully fetched 240 hourly records\n",
      "[16/29] Fetching JC (lat=40.35, lon=-74.65)...\n",
      "  ✓ Successfully fetched 240 hourly records\n",
      "[17/29] Fetching ME (lat=40.2, lon=-76.0)...\n",
      "  ✓ Successfully fetched 240 hourly records\n",
      "[18/29] Fetching OE (lat=41.1, lon=-81.25)...\n",
      "  ✓ Successfully fetched 240 hourly records\n",
      "[19/29] Fetching OVEC (lat=38.85, lon=-82.85)...\n",
      "  ✓ Successfully fetched 240 hourly records\n",
      "[20/29] Fetching PAPWR (lat=40.7, lon=-77.8)...\n",
      "  ✓ Successfully fetched 240 hourly records\n",
      "[21/29] Fetching PE (lat=40.0, lon=-75.2)...\n",
      "  ✓ Successfully fetched 240 hourly records\n",
      "[22/29] Fetching PEPCO (lat=38.9, lon=-76.95)...\n",
      "  ✓ Successfully fetched 240 hourly records\n",
      "[23/29] Fetching PLCO (lat=40.95, lon=-77.4)...\n",
      "  ✓ Successfully fetched 240 hourly records\n",
      "[24/29] Fetching PN (lat=41.15, lon=-77.8)...\n",
      "  ✓ Successfully fetched 240 hourly records\n",
      "[25/29] Fetching PS (lat=40.75, lon=-74.15)...\n",
      "  ✓ Successfully fetched 240 hourly records\n",
      "[26/29] Fetching RECO (lat=41.0, lon=-74.1)...\n",
      "  ✓ Successfully fetched 240 hourly records\n",
      "[27/29] Fetching SMECO (lat=38.4, lon=-76.7)...\n",
      "  ✓ Successfully fetched 240 hourly records\n",
      "[28/29] Fetching UGI (lat=40.25, lon=-75.65)...\n",
      "  ✓ Successfully fetched 240 hourly records\n",
      "[29/29] Fetching VMEU (lat=37.3, lon=-76.0)...\n",
      "  ✓ Successfully fetched 240 hourly records\n",
      "============================================================\n",
      "\n",
      "Data collection complete! Total zones processed: 29/29\n"
     ]
    }
   ],
   "source": [
    "# Initialize list to collect data from all zones\n",
    "all_data = []\n",
    "\n",
    "print(f\"Downloading weather data for {len(zone_coords)} load areas...\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for idx, row in zone_coords.iterrows():\n",
    "    zone = row['load_area']\n",
    "    lat = row['lat']\n",
    "    lon = row['lon']\n",
    "    \n",
    "    print(f\"[{idx+1}/{len(zone_coords)}] Fetching {zone} (lat={lat}, lon={lon})...\")\n",
    "    \n",
    "    # Fetch data for this zone\n",
    "    df = fetch_weather_data(lat, lon, start_date, end_date)\n",
    "    \n",
    "    if df is not None and not df.empty:\n",
    "        # Add load area identifier\n",
    "        df['load_area'] = zone\n",
    "        all_data.append(df)\n",
    "        print(f\"  ✓ Successfully fetched {len(df)} hourly records\")\n",
    "    else:\n",
    "        print(f\"  ✗ Failed to fetch data for {zone}\")\n",
    "    \n",
    "    # Small delay between requests to be respectful to the API\n",
    "    time.sleep(0.5)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nData collection complete! Total zones processed: {len(all_data)}/{len(zone_coords)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Combine Data and Convert to Eastern Time\n",
    "\n",
    "Combine all load area data and convert UTC timestamps to Eastern Time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined dataset shape: (6960, 7)\n",
      "Date range (UTC): 2025-11-21 00:00:00 to 2025-11-30 23:00:00\n",
      "\n",
      "Converting UTC to Eastern Time...\n",
      "✓ Time conversion complete\n",
      "\n",
      "Sample of data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime_beginning_ept</th>\n",
       "      <th>load_area</th>\n",
       "      <th>temp</th>\n",
       "      <th>humidity</th>\n",
       "      <th>dew_point</th>\n",
       "      <th>precip</th>\n",
       "      <th>wind</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11/20/2025 7:00:00 PM</td>\n",
       "      <td>AECO</td>\n",
       "      <td>6.4</td>\n",
       "      <td>72</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11/20/2025 8:00:00 PM</td>\n",
       "      <td>AECO</td>\n",
       "      <td>6.2</td>\n",
       "      <td>77</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11/20/2025 9:00:00 PM</td>\n",
       "      <td>AECO</td>\n",
       "      <td>6.8</td>\n",
       "      <td>79</td>\n",
       "      <td>3.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11/20/2025 10:00:00 PM</td>\n",
       "      <td>AECO</td>\n",
       "      <td>7.0</td>\n",
       "      <td>81</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11/20/2025 11:00:00 PM</td>\n",
       "      <td>AECO</td>\n",
       "      <td>7.3</td>\n",
       "      <td>76</td>\n",
       "      <td>3.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>11/21/2025 12:00:00 AM</td>\n",
       "      <td>AECO</td>\n",
       "      <td>7.2</td>\n",
       "      <td>76</td>\n",
       "      <td>3.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>11/21/2025 1:00:00 AM</td>\n",
       "      <td>AECO</td>\n",
       "      <td>7.3</td>\n",
       "      <td>83</td>\n",
       "      <td>4.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>11/21/2025 2:00:00 AM</td>\n",
       "      <td>AECO</td>\n",
       "      <td>7.2</td>\n",
       "      <td>86</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>11/21/2025 3:00:00 AM</td>\n",
       "      <td>AECO</td>\n",
       "      <td>7.4</td>\n",
       "      <td>82</td>\n",
       "      <td>4.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>11/21/2025 4:00:00 AM</td>\n",
       "      <td>AECO</td>\n",
       "      <td>7.6</td>\n",
       "      <td>81</td>\n",
       "      <td>4.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   datetime_beginning_ept load_area  temp  humidity  dew_point  precip  wind\n",
       "0   11/20/2025 7:00:00 PM      AECO   6.4        72        1.7     0.0   8.1\n",
       "1   11/20/2025 8:00:00 PM      AECO   6.2        77        2.5     0.0   6.3\n",
       "2   11/20/2025 9:00:00 PM      AECO   6.8        79        3.4     0.0   3.8\n",
       "3  11/20/2025 10:00:00 PM      AECO   7.0        81        4.0     0.0   4.2\n",
       "4  11/20/2025 11:00:00 PM      AECO   7.3        76        3.3     0.0   3.1\n",
       "5  11/21/2025 12:00:00 AM      AECO   7.2        76        3.3     0.0   0.5\n",
       "6   11/21/2025 1:00:00 AM      AECO   7.3        83        4.6     0.0   3.7\n",
       "7   11/21/2025 2:00:00 AM      AECO   7.2        86        5.0     0.0   3.5\n",
       "8   11/21/2025 3:00:00 AM      AECO   7.4        82        4.6     0.0   2.4\n",
       "9   11/21/2025 4:00:00 AM      AECO   7.6        81        4.6     0.0   0.8"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if all_data:\n",
    "    # Combine all data\n",
    "    combined_df = pd.concat(all_data, ignore_index=True)\n",
    "    \n",
    "    print(f\"Combined dataset shape: {combined_df.shape}\")\n",
    "    print(f\"Date range (UTC): {combined_df['datetime_beginning_utc'].min()} to {combined_df['datetime_beginning_utc'].max()}\")\n",
    "    \n",
    "    # Convert UTC to Eastern Time\n",
    "    print(\"\\nConverting UTC to Eastern Time...\")\n",
    "    combined_df['datetime_beginning_utc'] = pd.to_datetime(combined_df['datetime_beginning_utc'])\n",
    "    \n",
    "    # Create Eastern Time column\n",
    "    utc = pytz.UTC\n",
    "    eastern = pytz.timezone('US/Eastern')\n",
    "    et_time = combined_df['datetime_beginning_utc'].dt.tz_localize(utc).dt.tz_convert(eastern)\n",
    "    \n",
    "    # Format datetime in M/D/YYYY H:MM:SS AM/PM format\n",
    "    # Remove timezone info and format manually for cross-platform compatibility\n",
    "    et_time_no_tz = et_time.dt.tz_localize(None)\n",
    "    \n",
    "    # Try Unix format first, fall back to Windows format if needed\n",
    "    try:\n",
    "        combined_df['datetime_beginning_ept'] = et_time_no_tz.dt.strftime('%-m/%-d/%Y %-I:%M:%S %p')\n",
    "    except:\n",
    "        # Windows format\n",
    "        combined_df['datetime_beginning_ept'] = et_time_no_tz.dt.strftime('%#m/%#d/%Y %#I:%M:%S %p')\n",
    "    \n",
    "    # Remove UTC column and reorder\n",
    "    combined_df = combined_df.drop('datetime_beginning_utc', axis=1)\n",
    "    column_order = ['datetime_beginning_ept', 'load_area', 'temp', 'humidity', 'dew_point', 'precip', 'wind']\n",
    "    combined_df = combined_df[column_order]\n",
    "    \n",
    "    print(\"✓ Time conversion complete\")\n",
    "    \n",
    "    # Display sample\n",
    "    print(\"\\nSample of data:\")\n",
    "    display(combined_df.head(10))\n",
    "else:\n",
    "    print(\"No data was successfully downloaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "# Directory paths (relative to src/)\n",
    "FIGURES_DIR = \"../figures\"\n",
    "OUTPUT_DIR = \"../output\"\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(FIGURES_DIR, exist_ok=True)\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Target 29 load areas\n",
    "KEEP_AREAS = [\n",
    "    \"AECO\", \"AEPAPT\", \"AEPIMP\", \"AEPKPT\", \"AEPOPT\", \"AP\", \"BC\", \"CE\", \"DAY\", \"DEOK\",\n",
    "    \"DOM\", \"DPLCO\", \"DUQ\", \"EASTON\", \"EKPC\", \"JC\", \"ME\", \"OE\", \"OVEC\", \"PAPWR\",\n",
    "    \"PE\", \"PEPCO\", \"PLCO\", \"PN\", \"PS\", \"RECO\", \"SMECO\", \"UGI\", \"VMEU\"\n",
    "]\n",
    "\n",
    "# Best Model Finding dates (2025 test)\n",
    "TEST_START = '2025-11-20 00:00:00'\n",
    "TEST_END = '2025-11-29 23:00:00'\n",
    "\n",
    "# Rolling window for peak days\n",
    "WINDOW_SIZE = 10  # days\n",
    "NUM_PEAK_DAYS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= combined_df.copy()\n",
    "def parse_et(series):\n",
    "    \"\"\"Parse datetime with Eastern Time timezone handling\"\"\"\n",
    "    if pd.api.types.is_datetime64_any_dtype(series):\n",
    "        result = pd.to_datetime(series)\n",
    "        if result.dt.tz is None:\n",
    "            return result.dt.tz_localize('America/New_York', ambiguous='NaT', nonexistent='NaT')\n",
    "        return result\n",
    "    result = pd.to_datetime(series, format='%m/%d/%Y %I:%M:%S %p', errors='coerce')\n",
    "    mask = result.isna()\n",
    "    if mask.any():\n",
    "        result[mask] = pd.to_datetime(series[mask], errors='coerce')\n",
    "    return result.dt.tz_localize('America/New_York', ambiguous='NaT', nonexistent='NaT')\n",
    "\n",
    "df = df.rename(columns={\n",
    "    'datetime_beginning_ept': 'datetime',\n",
    "    'load_area': 'region',\n",
    "    'temp': 'temperature',\n",
    "    'precip': 'precipitation',\n",
    "    'wind': 'wind_speed'\n",
    "})\n",
    "\n",
    "# Keep only necessary columns\n",
    "keep_cols = ['datetime', 'region', 'temperature', 'humidity', 'precipitation', 'wind_speed']\n",
    "df = df[keep_cols]\n",
    "\n",
    "df['datetime'] = parse_et(df['datetime'])\n",
    "df = df.dropna(subset=['datetime'])\n",
    "df = df.sort_values('datetime').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_features(df):\n",
    "    \"\"\"Add temporal and calendar features\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # ===== TEMPORAL FEATURES =====\n",
    "    df['hour'] = df['datetime'].dt.hour\n",
    "    df['day_of_week'] = df['datetime'].dt.dayofweek  # Monday=0, Sunday=6\n",
    "    df['month'] = df['datetime'].dt.month\n",
    "    df['day_of_month'] = df['datetime'].dt.day\n",
    "    df['day_of_year'] = df['datetime'].dt.dayofyear\n",
    "    df['week_of_year'] = df['datetime'].dt.isocalendar().week\n",
    "    df['is_weekend'] = (df['day_of_week'] >= 5).astype(int)\n",
    "    \n",
    "    # Cyclical encoding\n",
    "    df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)\n",
    "    df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)\n",
    "    df['day_sin'] = np.sin(2 * np.pi * df['day_of_week'] / 7)\n",
    "    df['day_cos'] = np.cos(2 * np.pi * df['day_of_week'] / 7)\n",
    "    \n",
    "    # ===== HOLIDAY FEATURES =====\n",
    "    # Create US holiday calendar\n",
    "    us_holidays = holidays.US(years=range(2016, 2026))\n",
    "    df['date'] = df['datetime'].dt.date\n",
    "    df['is_holiday'] = df['date'].apply(lambda x: int(x in us_holidays))\n",
    "    \n",
    "    # Day before/after holiday\n",
    "    df['is_day_before_holiday'] = df['is_holiday'].shift(-24).fillna(0).astype(int)\n",
    "    df['is_day_after_holiday'] = df['is_holiday'].shift(24).fillna(0).astype(int)\n",
    "    \n",
    "    # Thanksgiving - Cooking load, midday peak\n",
    "    df['is_thanksgiving'] = df['date'].apply(\n",
    "        lambda x: int(1 if us_holidays.get(x) == 'Thanksgiving' else 0)\n",
    "    )\n",
    "    \n",
    "    # Christmas - Low commercial, high residential heating\n",
    "    df['is_christmas'] = df['date'].apply(\n",
    "        lambda x: int(1 if us_holidays.get(x) == 'Christmas Day' else 0)\n",
    "    )\n",
    "    \n",
    "    # New Year's Day - Late night/early morning shift\n",
    "    df['is_new_years'] = df['date'].apply(\n",
    "        lambda x: int(1 if us_holidays.get(x) == \"New Year's Day\" else 0)\n",
    "    )\n",
    "    \n",
    "    # July 4th - Summer, outdoor, evening grilling/fireworks\n",
    "    df['is_july4'] = df['date'].apply(\n",
    "        lambda x: int(1 if us_holidays.get(x) == 'Independence Day' else 0)\n",
    "    )\n",
    "    # Others\n",
    "    df['is_other_holiday'] = (df['is_holiday']-df['is_thanksgiving']-df['is_christmas']-df['is_new_years']-df['is_july4'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "df = add_features(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURE_COLS = [\n",
    "    # Temporal\n",
    "    'hour', 'day_of_week', 'month', 'day_of_month', 'day_of_year', 'week_of_year', 'is_weekend',\n",
    "    'hour_sin', 'hour_cos', 'day_sin', 'day_cos',\n",
    "    # Weather\n",
    "    'temperature', 'humidity', 'wind_speed', 'precipitation',\n",
    "    # 'is_holiday', 'is_day_before_holiday', 'is_day_after_holiday'\n",
    "    'is_thanksgiving', 'is_christmas', 'is_new_years', 'is_july4', 'is_other_holiday', 'is_day_before_holiday', 'is_day_after_holiday'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper functions defined\n"
     ]
    }
   ],
   "source": [
    "test_start = pd.to_datetime(TEST_START)\n",
    "test_end = pd.to_datetime(TEST_END)\n",
    "\n",
    "def prepare_data(df, test_start, test_end, FEATURE_COLS, region=None):\n",
    "    \"\"\"\n",
    "    Prepare train and test data for a given time period and region.\n",
    "    \"\"\"\n",
    "    # Filter by region if specified\n",
    "    if region is not None:\n",
    "        df = df[df['region'] == region].copy()\n",
    "    \n",
    "    # Convert string dates to datetime\n",
    "    test_start_dt = pd.Timestamp(test_start).tz_localize('America/New_York')\n",
    "    test_end_dt = pd.Timestamp(test_end).tz_localize('America/New_York')\n",
    "    \n",
    "    # Split data\n",
    "    test_data = df[(df['datetime'] >= test_start_dt) & (df['datetime'] <= test_end_dt)].copy()\n",
    "    \n",
    "    # Drop rows with missing lag features\n",
    "    test_data = test_data.dropna(subset=FEATURE_COLS)\n",
    "    \n",
    "    # Prepare X and y\n",
    "    X_test = test_data[FEATURE_COLS].values\n",
    "    \n",
    "    return X_test, test_data\n",
    "\n",
    "print(\"Helper functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading saved models...\n",
      "Loaded 29 regional models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating predictions: 100%|██████████████████| 29/29 [00:00<00:00, 610.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total predictions generated: 6,960\n",
      "\n",
      "Predictions shape: (11, 696)\n",
      "  Days: 11\n",
      "  Regions: 29\n",
      "  Hours: 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3D Array shape: (11, 29, 24)\n",
      "  Dimension 0 (days): 11\n",
      "  Dimension 1 (regions): 29\n",
      "  Dimension 2 (hours): 24\n",
      "\n",
      "Creating multi-index DataFrame...\n",
      "Predictions DataFrame shape: (319, 26)\n",
      "Columns: ['date', 'region', 'hour_00', 'hour_01', 'hour_02']...\n",
      "\n",
      "======================================================================\n",
      "Prediction generation complete!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Load the saved models\n",
    "regions= ['AECO', 'AEPAPT', 'AEPIMP', 'AEPKPT', 'AEPOPT', 'AP', 'BC', 'CE', 'DAY', 'DEOK',\n",
    "                  'DOM', 'DPLCO', 'DUQ', 'EASTON', 'EKPC', 'JC', 'ME', 'OE', 'OVEC', 'PAPWR',\n",
    "                  'PE', 'PEPCO', 'PLCO', 'PN', 'PS', 'RECO', 'SMECO', 'UGI', 'VMEU']\n",
    "models_dir = os.path.join(OUTPUT_DIR, 'trained_models')\n",
    "model_path = os.path.join(models_dir, 'hourly_load_models.pkl')\n",
    "\n",
    "print(\"Loading saved models...\")\n",
    "with open(model_path, 'rb') as f:\n",
    "    hourly_best_models = pickle.load(f)\n",
    "print(f\"Loaded {len(hourly_best_models)} regional models\")\n",
    "\n",
    "# Generate predictions for all regions\n",
    "predictions_list = []\n",
    "\n",
    "for region in tqdm(regions, desc=\"Generating predictions\"):\n",
    "    # Get model info for this region\n",
    "    model_info = hourly_best_models[region]\n",
    "    model = model_info['model']\n",
    "    feature_cols = model_info['feature_cols']\n",
    "    method = model_info['method']\n",
    "    \n",
    "    # Prepare prediction data\n",
    "    pred_data = df[df['region'] == region].copy()\n",
    "    \n",
    "    if len(pred_data) == 0:\n",
    "        print(f\"Warning: No data for {region} in prediction period\")\n",
    "        continue\n",
    "    \n",
    "    # Get features\n",
    "    X_pred = pred_data[feature_cols]\n",
    "    \n",
    "    y_pred = model.predict(X_pred)\n",
    "    y_pred = np.round(y_pred).astype(int)\n",
    "    # Store predictions with metadata\n",
    "    pred_df = pred_data[['date']].copy()\n",
    "    pred_df['region'] = region\n",
    "    pred_df['predicted_load'] = y_pred\n",
    "    pred_df['hour'] = pred_data['hour']\n",
    "    predictions_list.append(pred_df)\n",
    "\n",
    "# Combine all predictions\n",
    "all_predictions = pd.concat(predictions_list, ignore_index=True)\n",
    "\n",
    "print(f\"\\nTotal predictions generated: {len(all_predictions):,}\")\n",
    "\n",
    "# Create 3D structure: Pivot to get day x region x hour\n",
    "predictions_pivot = all_predictions.pivot_table(\n",
    "    index='date',\n",
    "    columns=['region', 'hour'],\n",
    "    values='predicted_load'\n",
    ")\n",
    "\n",
    "print(f\"\\nPredictions shape: {predictions_pivot.shape}\")\n",
    "print(f\"  Days: {len(predictions_pivot.index)}\")\n",
    "print(f\"  Regions: {len(regions)}\")\n",
    "print(f\"  Hours: 24\")\n",
    "\n",
    "# Alternative: Create a proper 3D array\n",
    "dates = sorted(all_predictions['date'].unique())\n",
    "hours = list(range(24))\n",
    "\n",
    "# Initialize 3D array: (days, regions, hours)\n",
    "predictions_3d = np.zeros((len(dates), len(regions), len(hours)))\n",
    "\n",
    "# Fill the 3D array\n",
    "for i, date in enumerate(dates):\n",
    "    for j, region in enumerate(regions):\n",
    "        for k, hour in enumerate(hours):\n",
    "            mask = (all_predictions['date'] == date) & \\\n",
    "                   (all_predictions['region'] == region) & \\\n",
    "                   (all_predictions['hour'] == hour)\n",
    "            \n",
    "            if mask.sum() > 0:\n",
    "                predictions_3d[i, j, k] = all_predictions.loc[mask, 'predicted_load'].values[0]\n",
    "            else:\n",
    "                predictions_3d[i, j, k] = np.nan\n",
    "\n",
    "print(f\"\\n3D Array shape: {predictions_3d.shape}\")\n",
    "print(f\"  Dimension 0 (days): {predictions_3d.shape[0]}\")\n",
    "print(f\"  Dimension 1 (regions): {predictions_3d.shape[1]}\")\n",
    "print(f\"  Dimension 2 (hours): {predictions_3d.shape[2]}\")\n",
    "\n",
    "all_predictions['predicted_load'] = all_predictions['predicted_load'].astype(int)\n",
    "# Create a more accessible DataFrame format\n",
    "print(\"\\nCreating multi-index DataFrame...\")\n",
    "predictions_df = all_predictions.pivot_table(\n",
    "    index=['date', 'region'],\n",
    "    columns='hour',\n",
    "    values='predicted_load'\n",
    ").reset_index()\n",
    "\n",
    "# Rename hour columns for clarity\n",
    "hour_cols = {i: f'hour_{i:02d}' for i in range(24)}\n",
    "predictions_df = predictions_df.rename(columns=hour_cols)\n",
    "\n",
    "print(f\"Predictions DataFrame shape: {predictions_df.shape}\")\n",
    "print(f\"Columns: {list(predictions_df.columns[:5])}...\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Prediction generation complete!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading saved peak hour prediction models...\n",
      "Loaded 29 regional models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting peak hours: 100%|███████████████████| 29/29 [00:00<00:00, 311.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating 2D DataFrame (regions × dates)...\n",
      "\n",
      "Peak Hour Predictions (hour of day, 0-23):\n",
      "        2025-11-20  2025-11-21  2025-11-22  2025-11-23  2025-11-24  \\\n",
      "AECO            19          17          18          18          18   \n",
      "AEPAPT          19          18          10           8           7   \n",
      "AEPIMP          19          10          19          18           7   \n",
      "AEPKPT          20           9           8           8           7   \n",
      "AEPOPT          19          10          18          18          18   \n",
      "AP              19          17          18          18           7   \n",
      "BC              19          17          18          18           7   \n",
      "CE              19          18          18          18          18   \n",
      "DAY             19           9          18          18          18   \n",
      "DEOK            19          17          18          18          18   \n",
      "DOM             19           7          17          18           7   \n",
      "DPLCO           19          17          17          18           7   \n",
      "DUQ             19          17          18          18          17   \n",
      "EASTON          19           7          10           7           7   \n",
      "EKPC            19          18           9           8           7   \n",
      "JC              19          17          18          18          18   \n",
      "ME              19          17          17          17          17   \n",
      "OE              19          10          18          18          18   \n",
      "OVEC            19          15           6          17          19   \n",
      "PAPWR           19          10          18          18           8   \n",
      "PE              19          17          17          17          17   \n",
      "PEPCO           20          17          18          18          17   \n",
      "PLCO            19          17          17          17           7   \n",
      "PN              19          10          17          18           7   \n",
      "PS              20          17          18          17          17   \n",
      "RECO            19          17          18          18          17   \n",
      "SMECO           19           7          18          19           7   \n",
      "UGI             19          17          17          17          18   \n",
      "VMEU            19          17          17          17          17   \n",
      "\n",
      "        2025-11-25  2025-11-26  2025-11-27  2025-11-28  2025-11-29  2025-11-30  \n",
      "AECO            17          17          18          18          18          18  \n",
      "AEPAPT           7          18           8           7           8           8  \n",
      "AEPIMP          10          18           8           7          18          18  \n",
      "AEPKPT           7          18           7           7           8           8  \n",
      "AEPOPT          18          18           7           7           8          18  \n",
      "AP               7          17          18          18           8          18  \n",
      "BC              17          17           7          18          18          18  \n",
      "CE              18          18          18          18          18          18  \n",
      "DAY             10          18          18          18           8          18  \n",
      "DEOK            18          18          18           7          18          18  \n",
      "DOM              7          17          18           7           7           7  \n",
      "DPLCO            7          17          11          18          18           7  \n",
      "DUQ             17          17          18          17          18          18  \n",
      "EASTON           7          18          18           7          18           8  \n",
      "EKPC            18          19           7           7           8           8  \n",
      "JC              17          17          18          18          18          18  \n",
      "ME              17          17          17          17          17          17  \n",
      "OE              18          18          18          17          18          18  \n",
      "OVEC             9           6          17          20           8          18  \n",
      "PAPWR            7          17          18           8          18          17  \n",
      "PE              17          17          17          17          17          18  \n",
      "PEPCO           17          17          17          17          19          18  \n",
      "PLCO             7          17          17          17          18          17  \n",
      "PN               7          18          10          17          17          18  \n",
      "PS              17          17          17          17          18          18  \n",
      "RECO            17          17          17          17          18          18  \n",
      "SMECO            7          18          19           7           7           7  \n",
      "UGI             17          17          11          18          17          17  \n",
      "VMEU            17          17          17          17          17          18  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the saved peak hour models\n",
    "regions = ['AECO', 'AEPAPT', 'AEPIMP', 'AEPKPT', 'AEPOPT', 'AP', 'BC', 'CE', 'DAY', 'DEOK',\n",
    "           'DOM', 'DPLCO', 'DUQ', 'EASTON', 'EKPC', 'JC', 'ME', 'OE', 'OVEC', 'PAPWR',\n",
    "           'PE', 'PEPCO', 'PLCO', 'PN', 'PS', 'RECO', 'SMECO', 'UGI', 'VMEU']\n",
    "\n",
    "models_dir = os.path.join(OUTPUT_DIR, 'trained_models')\n",
    "model_path = os.path.join(models_dir, 'peak_hour_models.pkl')\n",
    "\n",
    "print(\"Loading saved peak hour prediction models...\")\n",
    "with open(model_path, 'rb') as f:\n",
    "    peak_hour_best_models = pickle.load(f)\n",
    "print(f\"Loaded {len(peak_hour_best_models)} regional models\")\n",
    "\n",
    "\n",
    "# Dictionary to store peak hour predictions\n",
    "peak_hour_predictions = {}\n",
    "\n",
    "for region in tqdm(regions, desc=\"Predicting peak hours\"):\n",
    "    # Get model info for this region\n",
    "    model_info = peak_hour_best_models[region]\n",
    "    model = model_info['model']\n",
    "    feature_cols = model_info['feature_cols']\n",
    "    method = model_info['method']\n",
    "    \n",
    "    # Prepare prediction data for this region\n",
    "    pred_data = df[df['region'] == region].copy()\n",
    "    \n",
    "    if len(pred_data) == 0:\n",
    "        print(f\"Warning: No data for {region} in prediction period\")\n",
    "        continue\n",
    "    \n",
    "    # Get features\n",
    "    X_pred = pred_data[feature_cols]\n",
    "    \n",
    "    # Get predicted probabilities for being peak hour\n",
    "    y_pred_proba = model.predict_proba(X_pred)[:, 1]  # Probability of class 1 (is_peak_hour=True)\n",
    "    \n",
    "    # Add predictions to dataframe\n",
    "    pred_data['peak_hour_probability'] = y_pred_proba\n",
    "    \n",
    "    # For each date, find the hour with maximum probability\n",
    "    region_peak_hours = {}\n",
    "    for date in pred_data['date'].unique():\n",
    "        date_data = pred_data[pred_data['date'] == date]\n",
    "        # Find hour with maximum probability\n",
    "        peak_hour_idx = date_data['peak_hour_probability'].idxmax()\n",
    "        peak_hour = date_data.loc[peak_hour_idx, 'hour']\n",
    "        region_peak_hours[date] = int(peak_hour)\n",
    "    \n",
    "    peak_hour_predictions[region] = region_peak_hours\n",
    "\n",
    "# Convert to 2D DataFrame (regions × dates)\n",
    "print(\"\\nCreating 2D DataFrame (regions × dates)...\")\n",
    "\n",
    "# Get all unique dates\n",
    "all_dates = sorted(df['date'].unique())\n",
    "\n",
    "# Create DataFrame with regions as rows and dates as columns\n",
    "peak_hour_df = pd.DataFrame(peak_hour_predictions).T\n",
    "peak_hour_df = peak_hour_df.sort_index()  # Sort by region name\n",
    "\n",
    "# Ensure columns are sorted by date\n",
    "peak_hour_df = peak_hour_df[sorted(peak_hour_df.columns)]\n",
    "\n",
    "# Display the predictions\n",
    "print(\"\\nPeak Hour Predictions (hour of day, 0-23):\")\n",
    "print(peak_hour_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Daily aggregated data: 319 rows\n",
      "Date range: 2025-11-20 00:00:00 to 2025-11-30 00:00:00\n",
      "\n",
      "Features created (NO LOAD FEATURES - weather and date only)\n"
     ]
    }
   ],
   "source": [
    "feature_cols = [\n",
    "    # Temperature features\n",
    "    'temperature_max', 'temperature_min', 'temperature_mean', 'temperature_std', 'temp_range',\n",
    "    'hdd', 'cdd', 'is_very_hot', 'is_very_cold',\n",
    "    # Other weather\n",
    "    'humidity_max', 'humidity_min', 'humidity_mean',\n",
    "    'precipitation_sum', 'precipitation_max',\n",
    "    'wind_speed_max', 'wind_speed_mean',\n",
    "    # Temporal features\n",
    "    'month', 'day_of_week', 'day_of_month', 'week_of_year',\n",
    "    'is_weekend', 'is_holiday',\n",
    "    'month_sin', 'month_cos', 'dow_sin', 'dow_cos'\n",
    "]\n",
    "FP_COST = 1  # False Positive: predict peak when not peak\n",
    "FN_COST = 4  # False Negative: miss a peak day\n",
    "\n",
    "# Get US holidays\n",
    "us_holidays = holidays.US(years=range(2016, 2026))\n",
    "\n",
    "df['date'] = df['datetime'].dt.date\n",
    "\n",
    "# Aggregate to daily level - WEATHER ONLY (no load features except for labeling)\n",
    "daily_agg = df.groupby(['date', 'region']).agg({\n",
    "    'temperature': ['max', 'min', 'mean', 'std'],\n",
    "    'humidity': ['max', 'min', 'mean'],\n",
    "    'precipitation': ['sum', 'max'],\n",
    "    'wind_speed': ['max', 'mean']\n",
    "}).reset_index()\n",
    "\n",
    "# Flatten column names\n",
    "daily_agg.columns = ['_'.join(col).strip('_') if col[1] else col[0] for col in daily_agg.columns.values]\n",
    "\n",
    "# Convert date to datetime\n",
    "daily_agg['date'] = pd.to_datetime(daily_agg['date'])\n",
    "\n",
    "# Add temporal features\n",
    "daily_agg['year'] = daily_agg['date'].dt.year\n",
    "daily_agg['month'] = daily_agg['date'].dt.month\n",
    "daily_agg['day_of_week'] = daily_agg['date'].dt.dayofweek\n",
    "daily_agg['day_of_month'] = daily_agg['date'].dt.day\n",
    "daily_agg['week_of_year'] = daily_agg['date'].dt.isocalendar().week\n",
    "daily_agg['is_weekend'] = (daily_agg['day_of_week'] >= 5).astype(int)\n",
    "daily_agg['is_holiday'] = daily_agg['date'].apply(lambda x: x in us_holidays).astype(int)\n",
    "\n",
    "# Temperature features\n",
    "daily_agg['temp_range'] = daily_agg['temperature_max'] - daily_agg['temperature_min']\n",
    "\n",
    "# Heating/Cooling Degree Days (base 65°F)\n",
    "daily_agg['hdd'] = np.maximum(0, 65 - daily_agg['temperature_mean'])\n",
    "daily_agg['cdd'] = np.maximum(0, daily_agg['temperature_mean'] - 65)\n",
    "\n",
    "# Extreme temperature indicators\n",
    "daily_agg['is_very_hot'] = (daily_agg['temperature_max'] > 85).astype(int)\n",
    "daily_agg['is_very_cold'] = (daily_agg['temperature_min'] < 32).astype(int)\n",
    "\n",
    "# Cyclical encoding for temporal features\n",
    "daily_agg['month_sin'] = np.sin(2 * np.pi * daily_agg['month'] / 12)\n",
    "daily_agg['month_cos'] = np.cos(2 * np.pi * daily_agg['month'] / 12)\n",
    "daily_agg['dow_sin'] = np.sin(2 * np.pi * daily_agg['day_of_week'] / 7)\n",
    "daily_agg['dow_cos'] = np.cos(2 * np.pi * daily_agg['day_of_week'] / 7)\n",
    "\n",
    "print(f\"Daily aggregated data: {len(daily_agg):,} rows\")\n",
    "print(f\"Date range: {daily_agg['date'].min()} to {daily_agg['date'].max()}\")\n",
    "print(f\"\\nFeatures created (NO LOAD FEATURES - weather and date only)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = daily_agg[\n",
    "        (daily_agg['date'] >= test_start) & \n",
    "        (daily_agg['date'] <= test_end)\n",
    "    ].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading saved peak days prediction models...\n",
      "Loaded 29 regional models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting peak days: 100%|███████████████████| 29/29 [00:00<00:00, 1013.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating 2D DataFrame (regions × dates)...\n",
      "\n",
      "Peak Days Predictions DataFrame shape: (29, 10)\n",
      "  Rows (regions): 29\n",
      "  Columns (dates): 10\n",
      "\n",
      "Peak Days Predictions (0=non-peak day, 1=peak day):\n",
      "        2025-11-20  2025-11-21  2025-11-22  2025-11-23  2025-11-24  \\\n",
      "AECO             1           0           0           0           1   \n",
      "AEPAPT           0           0           0           0           1   \n",
      "AEPIMP           1           1           0           0           1   \n",
      "AEPKPT           0           0           0           0           0   \n",
      "AEPOPT           1           0           0           0           1   \n",
      "AP               1           0           0           0           1   \n",
      "BC               0           0           0           0           1   \n",
      "CE               1           0           0           0           1   \n",
      "DAY              1           1           0           0           1   \n",
      "DEOK             0           0           0           0           1   \n",
      "DOM              1           0           0           0           0   \n",
      "DPLCO            1           0           0           0           1   \n",
      "DUQ              1           1           0           0           1   \n",
      "EASTON           1           1           0           1           1   \n",
      "EKPC             0           0           0           0           0   \n",
      "JC               1           0           0           0           1   \n",
      "ME               1           0           0           0           1   \n",
      "OE               1           0           0           0           1   \n",
      "OVEC             1           1           1           1           1   \n",
      "PAPWR            1           1           0           0           1   \n",
      "PE               1           0           0           0           1   \n",
      "PEPCO            0           0           0           0           0   \n",
      "PLCO             1           1           0           0           1   \n",
      "PN               0           0           0           0           1   \n",
      "PS               1           0           0           0           1   \n",
      "RECO             1           1           0           0           1   \n",
      "SMECO            1           1           0           0           1   \n",
      "UGI              1           0           0           0           1   \n",
      "VMEU             0           0           0           0           1   \n",
      "\n",
      "        2025-11-25  2025-11-26  2025-11-27  2025-11-28  2025-11-29  \n",
      "AECO             1           0           0           1           1  \n",
      "AEPAPT           0           0           0           1           1  \n",
      "AEPIMP           1           0           1           1           0  \n",
      "AEPKPT           0           0           1           1           1  \n",
      "AEPOPT           0           0           0           1           0  \n",
      "AP               1           1           0           1           1  \n",
      "BC               1           0           1           1           1  \n",
      "CE               1           0           1           1           0  \n",
      "DAY              0           0           0           1           0  \n",
      "DEOK             1           1           1           1           1  \n",
      "DOM              0           0           0           1           1  \n",
      "DPLCO            1           0           0           1           1  \n",
      "DUQ              1           0           0           1           0  \n",
      "EASTON           1           0           0           1           1  \n",
      "EKPC             0           0           1           1           1  \n",
      "JC               1           0           0           1           1  \n",
      "ME               1           0           0           1           0  \n",
      "OE               0           0           0           1           0  \n",
      "OVEC             1           1           1           1           0  \n",
      "PAPWR            1           0           0           1           1  \n",
      "PE               1           1           0           1           0  \n",
      "PEPCO            0           1           0           1           0  \n",
      "PLCO             1           0           0           1           0  \n",
      "PN               1           0           0           1           0  \n",
      "PS               1           0           0           1           0  \n",
      "RECO             1           1           0           1           0  \n",
      "SMECO            1           0           0           1           1  \n",
      "UGI              1           0           0           1           1  \n",
      "VMEU             1           1           0           1           0  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the saved peak days models\n",
    "regions = ['AECO', 'AEPAPT', 'AEPIMP', 'AEPKPT', 'AEPOPT', 'AP', 'BC', 'CE', 'DAY', 'DEOK',\n",
    "           'DOM', 'DPLCO', 'DUQ', 'EASTON', 'EKPC', 'JC', 'ME', 'OE', 'OVEC', 'PAPWR',\n",
    "           'PE', 'PEPCO', 'PLCO', 'PN', 'PS', 'RECO', 'SMECO', 'UGI', 'VMEU']\n",
    "\n",
    "models_dir = os.path.join(OUTPUT_DIR, 'trained_models')\n",
    "model_path = os.path.join(models_dir, 'peak_days_models.pkl')\n",
    "\n",
    "print(\"Loading saved peak days prediction models...\")\n",
    "with open(model_path, 'rb') as f:\n",
    "    peak_days_best_models = pickle.load(f)\n",
    "print(f\"Loaded {len(peak_days_best_models)} regional models\")\n",
    "\n",
    "# Dictionary to store peak days predictions\n",
    "peak_days_predictions = {}\n",
    "\n",
    "for region in tqdm(regions, desc=\"Predicting peak days\"):\n",
    "    # Get model info for this region\n",
    "    model_info = peak_days_best_models[region]\n",
    "    model = model_info['model']\n",
    "    threshold = model_info['threshold']\n",
    "    \n",
    "    # Prepare prediction data for this region\n",
    "    pred_data = test_df[test_df['region'] == region].copy()\n",
    "    \n",
    "    if len(pred_data) == 0:\n",
    "        print(f\"Warning: No data for {region} in prediction period\")\n",
    "        continue\n",
    "    \n",
    "    # Get features (same columns used in training)\n",
    "    X_pred = pred_data[feature_cols]\n",
    "    \n",
    "    # Get prediction probabilities\n",
    "    y_pred_proba = model.predict_proba(X_pred)[:, 1]\n",
    "    \n",
    "    # Apply threshold to get binary predictions\n",
    "    y_pred = (y_pred_proba >= threshold).astype(int)\n",
    "    \n",
    "    # Add predictions to dataframe\n",
    "    pred_data['peak_day_pred'] = y_pred\n",
    "    \n",
    "    # Store predictions by date\n",
    "    region_predictions = {}\n",
    "    for idx, row in pred_data.iterrows():\n",
    "        region_predictions[row['date']] = row['peak_day_pred']\n",
    "    \n",
    "    peak_days_predictions[region] = region_predictions\n",
    "\n",
    "# Convert to 2D DataFrame (regions × dates)\n",
    "print(\"\\nCreating 2D DataFrame (regions × dates)...\")\n",
    "\n",
    "# Create DataFrame with regions as rows and dates as columns\n",
    "peak_days_df = pd.DataFrame(peak_days_predictions).T\n",
    "peak_days_df = peak_days_df.sort_index()  # Sort by region name\n",
    "\n",
    "# Ensure columns are sorted by date\n",
    "peak_days_df = peak_days_df[sorted(peak_days_df.columns)]\n",
    "\n",
    "# Convert DatetimeIndex to datetime.date objects (match peak_hour_df format)\n",
    "peak_days_df.columns = peak_days_df.columns.date \n",
    "\n",
    "print(f\"\\nPeak Days Predictions DataFrame shape: {peak_days_df.shape}\")\n",
    "print(f\"  Rows (regions): {peak_days_df.shape[0]}\")\n",
    "print(f\"  Columns (dates): {peak_days_df.shape[1]}\")\n",
    "\n",
    "# Display the predictions\n",
    "print(\"\\nPeak Days Predictions (0=non-peak day, 1=peak day):\")\n",
    "print(peak_days_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get tomorrow's date\n",
    "from datetime import datetime, timedelta\n",
    "tomorrow = datetime.now() + timedelta(days=1)\n",
    "tomorrow_date = tomorrow.date()\n",
    "tomorrow_str = tomorrow.strftime('%Y-%m-%d')\n",
    "\n",
    "# Read from existing prediction files\n",
    "predictions_dir = os.path.join(OUTPUT_DIR, 'predictions')\n",
    "\n",
    "# Build output in required format\n",
    "output_values = [f'\"{tomorrow_str}\"']\n",
    "\n",
    "# 1. HOURLY LOAD PREDICTIONS (L1_00 to L29_23) from predictions_df\n",
    "for region in regions:\n",
    "    region_data = predictions_df[(predictions_df['date'] == tomorrow_date) & \n",
    "                                  (predictions_df['region'] == region)]\n",
    "    \n",
    "    if len(region_data) > 0:\n",
    "        hour_cols = [f'hour_{i:02d}' for i in range(24)]\n",
    "        loads = region_data[hour_cols].values[0]\n",
    "        # loads = [int(round(load)) for load in loads]\n",
    "        output_values.extend([str(load) for load in loads])\n",
    "    else:\n",
    "        output_values.extend(['0'] * 24)\n",
    "\n",
    "# 2. PEAK HOUR PREDICTIONS (PH_1 to PH_29) from peak_hour_df\n",
    "for region in regions:\n",
    "    if tomorrow_date in peak_hour_df.columns and region in peak_hour_df.index:\n",
    "        peak_hour = int(peak_hour_df.loc[region, tomorrow_date])\n",
    "        output_values.append(str(peak_hour))\n",
    "    else:\n",
    "        output_values.append('0')\n",
    "\n",
    "# 3. PEAK DAY PREDICTIONS (PD_1 to PD_29) from peak_days_df\n",
    "for region in regions:\n",
    "    if tomorrow_date in peak_days_df.columns and region in peak_days_df.index:\n",
    "        peak_day = int(peak_days_df.loc[region, tomorrow_date])\n",
    "        output_values.append(str(peak_day))\n",
    "    else:\n",
    "        output_values.append('0')\n",
    "# Create the output line\n",
    "output_line = ','.join(output_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"2025-11-22\",820.0,800.0,779.0,765.0,772.0,787.0,835.0,852.0,863.0,913.0,887.0,890.0,889.0,903.0,929.0,948.0,1021.0,1112.0,1128.0,1103.0,1081.0,1068.0,995.0,941.0,3448.0,3385.0,3358.0,3340.0,3361.0,3431.0,3590.0,3743.0,3873.0,3942.0,3983.0,3947.0,3997.0,4029.0,4033.0,4102.0,4134.0,4149.0,4251.0,4310.0,4247.0,4235.0,4130.0,4031.0,2565.0,2519.0,2470.0,2447.0,2465.0,2492.0,2647.0,2717.0,2730.0,2791.0,2786.0,2823.0,2813.0,2792.0,2792.0,2787.0,2812.0,2831.0,2879.0,2899.0,2894.0,2856.0,2754.0,2642.0,529.0,528.0,527.0,533.0,543.0,545.0,569.0,589.0,597.0,609.0,614.0,616.0,625.0,616.0,616.0,618.0,622.0,623.0,656.0,664.0,679.0,689.0,676.0,659.0,5181.0,5032.0,4956.0,4947.0,4943.0,5071.0,5227.0,5532.0,5789.0,5993.0,5931.0,5903.0,5896.0,5833.0,5834.0,5841.0,5883.0,6165.0,6295.0,6319.0,6246.0,6259.0,6018.0,5799.0,4354.0,4244.0,4194.0,4198.0,4199.0,4297.0,4481.0,4767.0,4901.0,4918.0,5036.0,4968.0,5009.0,5049.0,5027.0,5155.0,5227.0,5352.0,5384.0,5424.0,5465.0,5393.0,5135.0,4893.0,2694.0,2625.0,2590.0,2587.0,2598.0,2622.0,2850.0,3016.0,3081.0,3127.0,3063.0,3046.0,3013.0,3018.0,3052.0,3200.0,3348.0,3551.0,3568.0,3592.0,3518.0,3409.0,3357.0,3171.0,9196.0,8854.0,8639.0,8647.0,8686.0,8692.0,8895.0,9305.0,9656.0,9745.0,9569.0,9417.0,9466.0,9415.0,9469.0,9400.0,9461.0,9720.0,9944.0,10154.0,10344.0,10170.0,9939.0,9520.0,1515.0,1446.0,1432.0,1402.0,1420.0,1463.0,1580.0,1712.0,1752.0,1800.0,1809.0,1764.0,1756.0,1746.0,1735.0,1734.0,1759.0,1836.0,1913.0,1912.0,1876.0,1846.0,1782.0,1717.0,2264.0,2188.0,2155.0,2122.0,2142.0,2180.0,2316.0,2440.0,2545.0,2629.0,2642.0,2649.0,2610.0,2637.0,2618.0,2653.0,2658.0,2799.0,2930.0,2937.0,2859.0,2828.0,2763.0,2618.0,9559.0,9328.0,9100.0,9194.0,9175.0,9363.0,9809.0,10275.0,10611.0,10661.0,10774.0,10883.0,10820.0,10919.0,10766.0,11155.0,11332.0,11572.0,11732.0,11694.0,11783.0,11417.0,11191.0,10675.0,1541.0,1524.0,1486.0,1499.0,1482.0,1543.0,1636.0,1724.0,1801.0,1836.0,1877.0,1869.0,1852.0,1842.0,1853.0,1906.0,1993.0,2069.0,2142.0,2127.0,2075.0,2067.0,2036.0,1919.0,1231.0,1204.0,1165.0,1171.0,1167.0,1178.0,1222.0,1290.0,1335.0,1378.0,1400.0,1390.0,1403.0,1404.0,1405.0,1423.0,1439.0,1512.0,1522.0,1520.0,1486.0,1454.0,1392.0,1340.0,21.0,21.0,20.0,21.0,20.0,21.0,24.0,25.0,27.0,27.0,27.0,27.0,27.0,28.0,27.0,27.0,28.0,31.0,31.0,32.0,31.0,31.0,29.0,29.0,1093.0,1065.0,1056.0,1053.0,1097.0,1102.0,1193.0,1267.0,1341.0,1381.0,1369.0,1423.0,1386.0,1394.0,1371.0,1383.0,1419.0,1427.0,1464.0,1479.0,1458.0,1461.0,1424.0,1341.0,1898.0,1825.0,1786.0,1767.0,1780.0,1858.0,1996.0,2086.0,2187.0,2262.0,2177.0,2183.0,2166.0,2143.0,2158.0,2258.0,2422.0,2682.0,2738.0,2655.0,2623.0,2536.0,2383.0,2215.0,1398.0,1346.0,1329.0,1329.0,1349.0,1374.0,1449.0,1564.0,1613.0,1639.0,1651.0,1657.0,1630.0,1613.0,1582.0,1618.0,1679.0,1773.0,1782.0,1806.0,1774.0,1696.0,1639.0,1565.0,5642.0,5493.0,5384.0,5363.0,5463.0,5524.0,5818.0,6334.0,6557.0,6509.0,6390.0,6383.0,6376.0,6387.0,6355.0,6338.0,6379.0,6616.0,6850.0,6902.0,6938.0,6835.0,6529.0,6245.0,33.0,32.0,32.0,32.0,39.0,33.0,40.0,35.0,35.0,35.0,34.0,35.0,34.0,34.0,34.0,34.0,35.0,36.0,37.0,37.0,38.0,37.0,37.0,37.0,455.0,441.0,432.0,430.0,431.0,440.0,463.0,499.0,513.0,525.0,533.0,535.0,534.0,526.0,532.0,527.0,538.0,557.0,574.0,568.0,558.0,549.0,520.0,492.0,3392.0,3349.0,3256.0,3255.0,3256.0,3318.0,3450.0,3751.0,3775.0,3903.0,4020.0,4086.0,4057.0,3973.0,3986.0,4142.0,4222.0,4498.0,4601.0,4534.0,4515.0,4389.0,4143.0,4027.0,2171.0,2055.0,2048.0,2030.0,2041.0,2089.0,2114.0,2231.0,2308.0,2378.0,2456.0,2436.0,2434.0,2378.0,2401.0,2489.0,2559.0,2606.0,2622.0,2726.0,2738.0,2664.0,2626.0,2514.0,3603.0,3543.0,3494.0,3469.0,3472.0,3637.0,3834.0,4149.0,4298.0,4332.0,4284.0,4259.0,4228.0,4217.0,4203.0,4237.0,4385.0,4659.0,4772.0,4835.0,4757.0,4638.0,4429.0,4228.0,1558.0,1511.0,1498.0,1505.0,1510.0,1542.0,1632.0,1728.0,1834.0,1851.0,1890.0,1901.0,1885.0,1894.0,1867.0,1872.0,1887.0,1996.0,1997.0,2024.0,1980.0,1924.0,1822.0,1753.0,3934.0,3749.0,3670.0,3651.0,3672.0,3750.0,3947.0,3982.0,4082.0,4179.0,4022.0,4058.0,4066.0,4053.0,4050.0,4180.0,4531.0,4879.0,4981.0,4944.0,4914.0,4861.0,4593.0,4387.0,123.0,116.0,114.0,113.0,113.0,116.0,122.0,128.0,133.0,132.0,130.0,131.0,130.0,128.0,131.0,132.0,141.0,152.0,157.0,161.0,160.0,154.0,146.0,135.0,296.0,285.0,278.0,278.0,274.0,284.0,311.0,333.0,354.0,366.0,374.0,376.0,370.0,373.0,385.0,384.0,388.0,415.0,434.0,439.0,424.0,430.0,412.0,406.0,95.0,91.0,87.0,88.0,88.0,92.0,97.0,107.0,115.0,117.0,121.0,124.0,121.0,120.0,120.0,122.0,128.0,133.0,136.0,136.0,135.0,130.0,122.0,115.0,63.0,61.0,60.0,59.0,59.0,61.0,64.0,66.0,66.0,66.0,63.0,62.0,63.0,69.0,69.0,72.0,74.0,76.0,76.0,76.0,75.0,74.0,70.0,67.0,18,10,19,8,18,18,18,18,18,18,17,17,18,10,9,18,17,18,6,18,17,18,17,17,18,18,18,17,17,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0\n"
     ]
    }
   ],
   "source": [
    "print(output_line)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
